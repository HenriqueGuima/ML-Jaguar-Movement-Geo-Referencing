{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from geopy.distance import geodesic\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12500\\3491684552.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/jaguar_movement_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location.long</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>individual.taxon.canonical.name</th>\n",
       "      <th>tag.local.identifier</th>\n",
       "      <th>individual.local.identifier (ID)</th>\n",
       "      <th>study.name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66096</th>\n",
       "      <td>66097.0</td>\n",
       "      <td>11/24/13 4:00</td>\n",
       "      <td>-56.693315</td>\n",
       "      <td>-17.241211</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>11158</td>\n",
       "      <td>59</td>\n",
       "      <td>Panthera_Pantanal-Jaguar-Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110339</th>\n",
       "      <td>110340.0</td>\n",
       "      <td>12/25/13 7:14</td>\n",
       "      <td>-48.671215</td>\n",
       "      <td>-15.919458</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>2314</td>\n",
       "      <td>89</td>\n",
       "      <td>Jaguars' Movement Pattern in a fragmente lands...</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57882</th>\n",
       "      <td>57883.0</td>\n",
       "      <td>8/20/09 23:30</td>\n",
       "      <td>-54.458698</td>\n",
       "      <td>-25.680085</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>Jaguar Conservation in Argentina</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116924</th>\n",
       "      <td>116925.0</td>\n",
       "      <td>5/16/13 20:01</td>\n",
       "      <td>-64.862937</td>\n",
       "      <td>-3.056006</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>1494</td>\n",
       "      <td>99</td>\n",
       "      <td>Jaguar_Mamiraua</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110688</th>\n",
       "      <td>110689.0</td>\n",
       "      <td>1/22/14 10:14</td>\n",
       "      <td>-48.675270</td>\n",
       "      <td>-16.035168</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>2314</td>\n",
       "      <td>89</td>\n",
       "      <td>Jaguars' Movement Pattern in a fragmente lands...</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Event_ID      timestamp  location.long  location.lat  \\\n",
       "66096    66097.0  11/24/13 4:00     -56.693315    -17.241211   \n",
       "110339  110340.0  12/25/13 7:14     -48.671215    -15.919458   \n",
       "57882    57883.0  8/20/09 23:30     -54.458698    -25.680085   \n",
       "116924  116925.0  5/16/13 20:01     -64.862937     -3.056006   \n",
       "110688  110689.0  1/22/14 10:14     -48.675270    -16.035168   \n",
       "\n",
       "       individual.taxon.canonical.name tag.local.identifier  \\\n",
       "66096                    Panthera onca                11158   \n",
       "110339                   Panthera onca                 2314   \n",
       "57882                    Panthera onca                    1   \n",
       "116924                   Panthera onca                 1494   \n",
       "110688                   Panthera onca                 2314   \n",
       "\n",
       "        individual.local.identifier (ID)  \\\n",
       "66096                                 59   \n",
       "110339                                89   \n",
       "57882                                 42   \n",
       "116924                                99   \n",
       "110688                                89   \n",
       "\n",
       "                                               study.name    country  \n",
       "66096                    Panthera_Pantanal-Jaguar-Project     Brazil  \n",
       "110339  Jaguars' Movement Pattern in a fragmente lands...     Brazil  \n",
       "57882                    Jaguar Conservation in Argentina  Argentina  \n",
       "116924                                    Jaguar_Mamiraua     Brazil  \n",
       "110688  Jaguars' Movement Pattern in a fragmente lands...     Brazil  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 15000\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/jaguar_movement_data.csv')\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "data = data.sample(SAMPLE_SIZE) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event_ID                            0\n",
       "timestamp                           0\n",
       "location.long                       0\n",
       "location.lat                        0\n",
       "individual.taxon.canonical.name     0\n",
       "tag.local.identifier                0\n",
       "individual.local.identifier (ID)    0\n",
       "study.name                          0\n",
       "country                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cp = data.copy()\n",
    "data_cp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix features names Event_ID,timestamp,location.long,location.lat,individual.taxon.canonical.name,tag.local.identifier,individual.local.identifier (ID),study.name,country\n",
    "data_cp = data_cp.rename(columns={'Event_ID':'event_id','timestamp':'timestamp','location.long':'location_long','location.lat':'location_lat','individual.taxon.canonical.name':'individual_taxon_canonical_name','tag.local.identifier':'tag_local_identifier','individual.local.identifier (ID)':'individual_local_identifier_ID','study.name':'study_name','country':'country'})\n",
    "\n",
    "# Print features names\n",
    "data_cp.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12500\\3021486648.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "# Parsing the timestamp column into a datetime format\n",
    "data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n",
    "\n",
    "# Extract time of day, day of the week, month, etc., for temporal features.\n",
    "data_cp['hour'] = data_cp['timestamp'].dt.hour\n",
    "data_cp['day'] = data_cp['timestamp'].dt.day\n",
    "data_cp['month'] = data_cp['timestamp'].dt.month\n",
    "data_cp['year'] = data_cp['timestamp'].dt.year\n",
    "data_cp['dayofweek'] = data_cp['timestamp'].dt.dayofweek\n",
    "data_cp['date'] = data_cp['timestamp'].dt.date\n",
    "\n",
    "# Show unique values for day, month and year\n",
    "# print(data_cp['day'].unique())\n",
    "# print(data_cp['month'].unique())\n",
    "# print(data_cp['year'].unique())\n",
    "# print(data_cp['country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country', 'hour',\n",
       "       'day', 'month', 'year', 'dayofweek', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cp = data_cp.drop(['event_id', 'individual_local_identifier_ID', 'tag_local_identifier'], axis=1)\n",
    "\n",
    "data_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances between consecutive events.\n",
    "### Calculating the distance between points using the Haversine Formula and group the data by tag_local_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12500\\2715588214.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location_long</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>individual_taxon_canonical_name</th>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th>individual_local_identifier_ID</th>\n",
       "      <th>study_name</th>\n",
       "      <th>country</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131870</th>\n",
       "      <td>131871.0</td>\n",
       "      <td>2015-10-12 00:00:00</td>\n",
       "      <td>-57.503400</td>\n",
       "      <td>-16.881275</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131871</th>\n",
       "      <td>131872.0</td>\n",
       "      <td>2015-10-12 01:00:00</td>\n",
       "      <td>-57.503355</td>\n",
       "      <td>-16.881304</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131885</th>\n",
       "      <td>131886.0</td>\n",
       "      <td>2015-10-12 15:00:00</td>\n",
       "      <td>-57.500259</td>\n",
       "      <td>-16.882043</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131920</th>\n",
       "      <td>131921.0</td>\n",
       "      <td>2015-10-14 03:00:00</td>\n",
       "      <td>-57.497704</td>\n",
       "      <td>-16.889077</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131927</th>\n",
       "      <td>131928.0</td>\n",
       "      <td>2015-10-14 10:00:00</td>\n",
       "      <td>-57.497605</td>\n",
       "      <td>-16.889124</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event_id           timestamp  location_long  \\\n",
       "tag_local_identifier                                                       \n",
       "35957                131870  131871.0 2015-10-12 00:00:00     -57.503400   \n",
       "                     131871  131872.0 2015-10-12 01:00:00     -57.503355   \n",
       "                     131885  131886.0 2015-10-12 15:00:00     -57.500259   \n",
       "                     131920  131921.0 2015-10-14 03:00:00     -57.497704   \n",
       "                     131927  131928.0 2015-10-14 10:00:00     -57.497605   \n",
       "\n",
       "                             location_lat individual_taxon_canonical_name  \\\n",
       "tag_local_identifier                                                        \n",
       "35957                131870    -16.881275                   Panthera onca   \n",
       "                     131871    -16.881304                   Panthera onca   \n",
       "                     131885    -16.882043                   Panthera onca   \n",
       "                     131920    -16.889077                   Panthera onca   \n",
       "                     131927    -16.889124                   Panthera onca   \n",
       "\n",
       "                            tag_local_identifier  \\\n",
       "tag_local_identifier                               \n",
       "35957                131870                35957   \n",
       "                     131871                35957   \n",
       "                     131885                35957   \n",
       "                     131920                35957   \n",
       "                     131927                35957   \n",
       "\n",
       "                             individual_local_identifier_ID     study_name  \\\n",
       "tag_local_identifier                                                         \n",
       "35957                131870                             117  Jaguar_Taiama   \n",
       "                     131871                             117  Jaguar_Taiama   \n",
       "                     131885                             117  Jaguar_Taiama   \n",
       "                     131920                             117  Jaguar_Taiama   \n",
       "                     131927                             117  Jaguar_Taiama   \n",
       "\n",
       "                            country  hour  day  month  year  dayofweek  \\\n",
       "tag_local_identifier                                                     \n",
       "35957                131870  Brazil     0   12     10  2015          0   \n",
       "                     131871  Brazil     1   12     10  2015          0   \n",
       "                     131885  Brazil    15   12     10  2015          0   \n",
       "                     131920  Brazil     3   14     10  2015          2   \n",
       "                     131927  Brazil    10   14     10  2015          2   \n",
       "\n",
       "                                   date  distance  \n",
       "tag_local_identifier                               \n",
       "35957                131870  2015-10-12       NaN  \n",
       "                     131871  2015-10-12     0.006  \n",
       "                     131885  2015-10-12     0.340  \n",
       "                     131920  2015-10-14     0.828  \n",
       "                     131927  2015-10-14     0.012  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fe = data_cp.copy()\n",
    "\n",
    "data_fe['timestamp'] = pd.to_datetime(data_fe['timestamp'])\n",
    "\n",
    "# Define Haversine formula for distance calculation\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "# Function to calculate distance only for the same animal\n",
    "def calculate_distances_for_animal(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    group['distance'] = haversine(\n",
    "        group['location_lat'].shift(),\n",
    "        group['location_long'].shift(),\n",
    "        group['location_lat'],\n",
    "        group['location_long']\n",
    "    )\n",
    "    return group\n",
    "\n",
    "data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n",
    "\n",
    "# Convert distance to kilometers\n",
    "data_fe['distance'] = data_fe['distance'] / 1000\n",
    "data_fe['distance'] = data_fe['distance'].round(3)\n",
    "\n",
    "data_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Labeling the Data\n",
    " Behavior Labeling:\n",
    " Manually label a subset of data with different behaviors (e.g., movement, hunting, resting).\n",
    " Consider cross-referencing domain knowledge or complementary data to infer labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movement > 0.5: 'hunting' (highest priority)\n",
    "\n",
    "0.1 < velocity ≤ 0.5: 'movement'\n",
    "\n",
    "0.0 < velocity ≤ 0.1: 'slowing down'\n",
    "\n",
    "= 0.0 'stopped'\n",
    "\n",
    "< 0.0: 'movement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             velocity  movement\n",
      "tag_local_identifier                           \n",
      "35957                131871    0.0017  movement\n",
      "                     131885    0.0067  movement\n",
      "                     131920    0.0064  movement\n",
      "                     131927    0.0005  movement\n",
      "                     131937    0.0188  movement\n",
      "...                               ...       ...\n",
      "TIAGO                106142    0.0034  movement\n",
      "                     106149    0.0050  movement\n",
      "                     106152    0.0071  movement\n",
      "                     106154    0.0005  movement\n",
      "                     106158    0.0049  movement\n",
      "\n",
      "[14887 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat).\n",
    "# time_diff = (data_cp['timestamp'] - data_cp['timestamp'].shift()).dt.total_seconds()\n",
    "\n",
    "# Ensuring the distance is in meters and the time differece is in seconds\n",
    "time_diff = data_fe['time_diff'] = data_fe['timestamp'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "data_fe['distance'] = data_fe['distance'] * 1000\n",
    "time_diff_seconds = data_fe['time_diff'] * 3600\n",
    "\n",
    "# print(data_fe['time_diff'].head())\n",
    "\n",
    "data_fe['velocity'] = data_fe['distance'] / time_diff_seconds\n",
    "data_fe['velocity'] = data_fe['velocity'].round(4)\n",
    "data_fe = data_fe.dropna(subset=[ 'distance', 'velocity'])\n",
    "\n",
    "data_fe['direction'] = np.arctan2(data_fe['location_long'] - data_fe['location_long'].shift(), data_fe['location_lat'] - data_fe['location_lat'].shift())\n",
    "\n",
    "data_fe['movement'] = np.where(\n",
    "    data_fe['velocity'] > 0.5, 'hunting',\n",
    "    np.where(\n",
    "        data_fe['velocity'] > 0.0, 'movement',\n",
    "        # np.where(\n",
    "        #     data_fe['velocity'] > 0.0, 'slowing down',\n",
    "            np.where(\n",
    "                data_fe['velocity'] == 0.0, 'stopped', \n",
    "                'movement'  # Velocity < 0.0\n",
    "            )\n",
    "        # )\n",
    "    )\n",
    ")\n",
    "# data_cp = data_cp.dropna()\n",
    "print(data_fe[['velocity', 'movement']])\n",
    "# print(data_fe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive acceleration or changes in movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate acceleration as the change in velocity over time\n",
    "data_fe['acceleration'] = data_fe['velocity'].diff() / data_fe['time_diff']\n",
    "\n",
    "# Calculate change in direction\n",
    "data_fe['change_in_direction'] = data_fe['direction'].diff()\n",
    "\n",
    "# Drop rows with NaN values in acceleration and change_in_direction\n",
    "data_fe = data_fe.dropna(subset=['acceleration', 'change_in_direction'])\n",
    "\n",
    "# Round the values for better readability\n",
    "data_fe['acceleration'] = data_fe['acceleration'].round(4)\n",
    "data_fe['change_in_direction'] = data_fe['change_in_direction'].round(4)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "data_fe[['acceleration', 'change_in_direction']].head()\n",
    "data_fe.to_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/jaguar_movement_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109]\n"
     ]
    }
   ],
   "source": [
    "# Encode tag_local_identifier to numerical values\n",
    "\n",
    "data_fe['tag_local_identifier'] = pd.Categorical(data_fe['tag_local_identifier'])\n",
    "data_fe['tag_local_identifier'] = data_fe['tag_local_identifier'].cat.codes\n",
    "\n",
    "print(data_fe['tag_local_identifier'].unique())\n",
    "# data_encoded = pd.get_dummies(data_fe, columns=['tag_local_identifier'])\n",
    "# data_encoded.columns\n",
    "# data_encoded['tag_local_identifier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to get temporal features from Paraguay, Brazil, Costa Rica, Argentina and Mexico between 1/1/1999 and 31/12/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.8)\n",
      "(553652, 127)\n",
      "(4169, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get World  Climate data\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"christopherlemke/monthly-climat-reports-from-stations-worldwide\")\n",
    "\n",
    "reports_data = pd.read_csv(path + '/dwd-cdc_CLIMAT_reports_stations_ww.csv')\n",
    "stations_data = pd.read_csv(path + '/dwd-cdc_station_data_ww.csv')\n",
    "\n",
    "# Save the data to a csv file\n",
    "reports_data.to_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/monthly-climat-reports-from-stations-worldwide.csv', index=False)\n",
    "stations_data.to_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/dwd-cdc_station_data_ww.csv', index=False)\n",
    "\n",
    "print(reports_data.shape)\n",
    "print(stations_data.shape)\n",
    "\n",
    "reports_data = reports_data.sample(SAMPLE_SIZE)\n",
    "# stations_data = stations_data.sample(SAMPLE_SIZE)\n",
    "\n",
    "# print(reports_data.head())\n",
    "# print(stations_data.head())\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Reports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farenheit_to_celsius(farenheit):\n",
    "    return ((farenheit - 32) * 5.0)/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Engineering\n",
    " \n",
    " Environmental Features:\n",
    " Integrate meteorological data (e.g., temperature, rainfall)\n",
    "\n",
    " Temporal Features:\n",
    " Extract time of day, day of week, and season from the timestamp.\n",
    " Identify specific behavioral periods like night vs. day.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest_daily_amount_precipitation_month_tenths_mm    4613\n",
      "mean_daily_maximum_air_temperature                       0\n",
      "mean_daily_minimum_air_temperature                       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>station_id</th>\n",
       "      <th>G1</th>\n",
       "      <th>G1.1</th>\n",
       "      <th>G1.2</th>\n",
       "      <th>sn</th>\n",
       "      <th>monthly_mean_air_temperature</th>\n",
       "      <th>G1.3</th>\n",
       "      <th>sn.1</th>\n",
       "      <th>...</th>\n",
       "      <th>iw</th>\n",
       "      <th>fx</th>\n",
       "      <th>yfx</th>\n",
       "      <th>G4.6</th>\n",
       "      <th>Dts</th>\n",
       "      <th>Dgr</th>\n",
       "      <th>G4.7</th>\n",
       "      <th>iy</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304949</th>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>16134</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.670000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359570</th>\n",
       "      <td>2003</td>\n",
       "      <td>5</td>\n",
       "      <td>47426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.890000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337445</th>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>89345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.560000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474158</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>92035</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.603193</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274018</th>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>94842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
       "304949  2016      7       16134  1.0   2.0   3.0  0.0   \n",
       "359570  2003      5       47426  1.0   2.0   3.0  0.0   \n",
       "337445  2018     10       89345  1.0   NaN   3.0  1.0   \n",
       "474158  2020      1       92035  1.0   2.0   3.0  NaN   \n",
       "274018  2014      6       94842  1.0   2.0   3.0  0.0   \n",
       "\n",
       "        monthly_mean_air_temperature  G1.3  sn.1  ...   iw     fx   yfx  G4.6  \\\n",
       "304949                     56.670000   4.0   0.0  ...  4.0  650.0  15.0   6.0   \n",
       "359570                     33.890000   4.0   0.0  ...  1.0  176.0   8.0   6.0   \n",
       "337445                     95.560000   4.0   1.0  ...  1.0  139.0  25.0   NaN   \n",
       "474158                     76.603193   4.0   0.0  ...  NaN    NaN   NaN   NaN   \n",
       "274018                     50.000000   4.0   0.0  ...  1.0  355.0  24.0   6.0   \n",
       "\n",
       "        Dts  Dgr  G4.7   iy    Gx    Gn  \n",
       "304949  5.0  1.0   NaN  NaN   NaN   NaN  \n",
       "359570  0.0  0.0   NaN  NaN   NaN   NaN  \n",
       "337445  NaN  NaN   NaN  NaN   NaN   NaN  \n",
       "474158  NaN  NaN   NaN  NaN   NaN   NaN  \n",
       "274018  1.0  1.0   7.0  1.0  23.0  23.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove columns\n",
    "reports_data_dropped = reports_data.drop(['Po', 'Po.1', 'P', 'P.1', 'T.1', 'st', 'st.1', \n",
    "                                          'Tx.1', 'Tn.1', \n",
    "                                          'e', 'e.1', \n",
    "                                          'R1.1', 'nr.1', 'S1.1', 'Rd', 'mp', 'mT', 'mTx', 'mTn', 'me', 'mR', 'mS', \n",
    "                                            #   'sn', 'sn.1', 'sn.2', 'sn.3', 'sn.4', 'sn.5', 'sn.6', 'sn.7', 'sn.8', 'sn.9',\n",
    "                                            # 'G1', 'G1.1', 'G1.2', 'G1.3', 'G1.4', 'G1.5', 'G1.6', 'G1.7', 'G1.8',\n",
    "                                            # 'G2', 'G2.1', 'G2.2', 'G2.3', 'G2.4', 'G2.5', 'G2.6', 'G2.7', 'G2.8', 'G2.9',\n",
    "                                            # 'G3', 'G3.1', 'G3.2', 'G3.3', 'G3.4', 'G3.5', 'G3.6', 'G3.7', 'G3.8', 'G3.9',\n",
    "                                            # 'G4', 'G4.1', 'G4.2', 'G4.3', 'G4.4', 'G4.5', 'G4.6', 'G4.7',\n",
    "                                            'Yb', 'Yc', 'P', 'YP', 'YR', 'YS', 'YT', 'YTx',\n",
    "                                            'Ye', 'G4', 'Txd', 'yx', 'Tnd', 'Tax', 'Tan' ], axis=1)\n",
    "\n",
    "# Drop columns with no data\n",
    "reports_data_dropped = reports_data_dropped.dropna(axis=1, how='all')\n",
    "\n",
    "# Rename columns\n",
    "reports_data_renamed = reports_data_dropped.rename(columns={'IIiii':'station_id', 'T':'monthly_mean_air_temperature', \n",
    "                                            'Tx':'mean_daily_maximum_air_temperature', \n",
    "                                            'Tn':'mean_daily_minimum_air_temperature', \n",
    "                                            'R1':'total_precipitation_month', 'S1':'total_sunshine_month', \n",
    "                                            'ps':'percentage_total_sunshine_duration_relative_normal', \n",
    "                                            'P0':'monthly_mean_pressure_station_level', 'e':'mean_vapor_pressure_month', \n",
    "                                            'nr':'number_days_month_precipitation', \n",
    "                                            'yP':'missing_years_air_pressure', 'yR':'missing_years_precipitation', 'yS':'missing_years_sunshine_duration', \n",
    "                                            'yT':'missing_years_mean_air_temperature', 'yTx':'missing_years_mean_extreme_air_temperature', 'ye':'missing_years_vapor_pressure', \n",
    "                                            'T25':'days_month_maximum_air_temperature_25', 'T30':'days_month_maximum_air_temperature_30', \n",
    "                                            'T35':'days_month_maximum_air_temperature_35', 'T40':'days_month_maximum_air_temperature_40', \n",
    "                                            'Tn0':'days_month_minimum_air_temperature_0', 'Tx0':'days_month_maximum_air_temperature_0', 'R01':'days_month_precipitation_1', \n",
    "                                            'R05':'days_month_precipitation_5', 'R10':'days_month_precipitation_10', 'R50':'days_month_precipitation_50', \n",
    "                                            'R100':'days_month_precipitation_100', 'R150':'days_month_precipitation_150', 's00':'days_month_snow_depth_0', \n",
    "                                            's01':'days_month_snow_depth_1', 's10':'days_month_snow_depth_10', 's50':'days_month_snow_depth_50', 'f10':'days_month_wind_speed_10', \n",
    "                                            'f20':'days_month_wind_speed_20', 'f30':'days_month_wind_speed_30', 'V1':'days_month_visibility_50', 'V2':'days_month_visibility_100', \n",
    "                                            'V3':'days_month_visibility_1000', 'yn': 'day_lowest_daily_mean_air_temperature_month', 'yax': 'day_highest_daily_mean_air_temperature_month', 'yan':'day_lowest_air_tempreature_month',\n",
    "                                            'Rx':'highest_daily_amount_precipitation_month_tenths_mm',  \n",
    "                                            # 'yr':'day_highest_daily_amount_precipitation_month'\n",
    "                                            })\n",
    "\n",
    "# Remove rows with missing values in [year]\n",
    "reports_data_renamed = reports_data_renamed.dropna(subset=['year'])\n",
    "\n",
    "# Enconde Year, Month, and Station ID to integer\n",
    "reports_data_renamed['year'] = reports_data_renamed['year'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['month'] = reports_data_renamed['month'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['station_id'] = reports_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['sn'] = reports_data_renamed['sn'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "# print(reports_data_renamed[['mean_daily_maximum_air_temperature']].head())\n",
    "\n",
    "# Convert temperature features to Celsius\n",
    "# °C = (°F - 32) × 5/9\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = farenheit_to_celsius(reports_data_renamed['monthly_mean_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] =  farenheit_to_celsius(reports_data_renamed['mean_daily_maximum_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = farenheit_to_celsius(reports_data_renamed['mean_daily_minimum_air_temperature']).round(2)\n",
    "\n",
    "# print(reports_data_renamed[['mean_daily_maximum_air_temperature']].head())\n",
    "\n",
    "# Check for missing values in temperature features\n",
    "# print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = reports_data_renamed['monthly_mean_air_temperature'].fillna(reports_data_renamed['monthly_mean_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] = reports_data_renamed['mean_daily_maximum_air_temperature'].fillna(reports_data_renamed['mean_daily_maximum_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = reports_data_renamed['mean_daily_minimum_air_temperature'].fillna(reports_data_renamed['mean_daily_minimum_air_temperature'].mean())\n",
    "\n",
    "print(reports_data_renamed[['highest_daily_amount_precipitation_month_tenths_mm', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# Show null values\n",
    "# print(reports_data_renamed.isnull().sum())\n",
    "reports_data_renamed.to_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/monthly-climat-reports-from-stations-worldwide-cleaned.csv', index=False)\n",
    "\n",
    "reports_data_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01001' '01005' '01007' ... '99090' '99092' '99113']\n"
     ]
    }
   ],
   "source": [
    "# stations_data.dtypes\n",
    "\n",
    "# Rename columns\n",
    "# 0: Station ID, 1: Station Name, 2: Latitude, 3: Longitude, 4:Height, 5: Country\n",
    "stations_data_renamed = stations_data.rename(columns={'0':'station_id', '1':'station_name', '2':'latitude', '3':'longitude', '4':'height', '5':'country'})\n",
    "stations_data_renamed['station_id'] = stations_data_renamed['station_id'].str.strip()\n",
    "stations_data_renamed = stations_data_renamed[:-1]\n",
    "\n",
    "print(stations_data_renamed['station_id'].unique())\n",
    "\n",
    "\n",
    "stations_data_renamed['station_id'] = (\n",
    "    stations_data_renamed['station_id']\n",
    "    .str.replace(r'\\D', '', regex=True)  # Remove all non-digit characters\n",
    "    .pipe(pd.to_numeric, errors='coerce')  # Convert to numeric, invalid become NaN\n",
    "    .astype('Int64')  # Convert to pandas' nullable Int64 type\n",
    ")\n",
    "\n",
    "\n",
    "# stations_data_renamed['station_id'] = stations_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "# stations_data_renamed.head()\n",
    "# stations_data_renamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         432.0\n",
      "1         195.0\n",
      "2           NaN\n",
      "3           NaN\n",
      "4         382.0\n",
      "          ...  \n",
      "14024     564.0\n",
      "14025      27.0\n",
      "14026     340.0\n",
      "14027    1304.0\n",
      "14028       5.0\n",
      "Name: highest_daily_amount_precipitation_month_tenths_mm, Length: 14029, dtype: float64\n",
      "=== REPORTS DATA ===\n",
      "        year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
      "304949  2016      7       16134  1.0   2.0   3.0  0.0   \n",
      "359570  2003      5       47426  1.0   2.0   3.0  0.0   \n",
      "337445  2018     10       89345  1.0   NaN   3.0  1.0   \n",
      "474158  2020      1       92035  1.0   2.0   3.0  NaN   \n",
      "274018  2014      6       94842  1.0   2.0   3.0  0.0   \n",
      "\n",
      "        monthly_mean_air_temperature  G1.3  sn.1  ...   iw     fx   yfx  G4.6  \\\n",
      "304949                     56.670000   4.0   0.0  ...  4.0  650.0  15.0   6.0   \n",
      "359570                     33.890000   4.0   0.0  ...  1.0  176.0   8.0   6.0   \n",
      "337445                     95.560000   4.0   1.0  ...  1.0  139.0  25.0   NaN   \n",
      "474158                     76.603193   4.0   0.0  ...  NaN    NaN   NaN   NaN   \n",
      "274018                     50.000000   4.0   0.0  ...  1.0  355.0  24.0   6.0   \n",
      "\n",
      "        Dts  Dgr  G4.7   iy    Gx    Gn  \n",
      "304949  5.0  1.0   NaN  NaN   NaN   NaN  \n",
      "359570  0.0  0.0   NaN  NaN   NaN   NaN  \n",
      "337445  NaN  NaN   NaN  NaN   NaN   NaN  \n",
      "474158  NaN  NaN   NaN  NaN   NaN   NaN  \n",
      "274018  1.0  1.0   7.0  1.0  23.0  23.0  \n",
      "\n",
      "[5 rows x 91 columns]\n",
      "=== STATIONS DATA ===\n",
      "   station_id              station_name  \\\n",
      "0        1001                 Jan Mayen   \n",
      "1        1005             Isfjord Radio   \n",
      "2        1007                Ny-Alesund   \n",
      "3        1008                  Svalbard   \n",
      "4        1025            Tromso/Langnes   \n",
      "\n",
      "                                      latitude   longitude   height  \\\n",
      "0                                        70.94      -08.67        9   \n",
      "1                                        78.06       13.63        9   \n",
      "2                                        78.92       11.93        8   \n",
      "3                                        78.25       15.50       27   \n",
      "4                                        69.68       18.91        9   \n",
      "\n",
      "                                             country  \n",
      "0   Norway                                       ...  \n",
      "1   Norway                                       ...  \n",
      "2   Norway                                       ...  \n",
      "3   Norway                                       ...  \n",
      "4   Norway                                       ...  \n",
      "=== DATA FE ===\n",
      "                             event_id           timestamp  location_long  \\\n",
      "tag_local_identifier                                                       \n",
      "35957                131920  131921.0 2015-10-14 03:00:00     -57.497704   \n",
      "                     131927  131928.0 2015-10-14 10:00:00     -57.497605   \n",
      "                     131937  131938.0 2015-10-14 21:00:00     -57.490651   \n",
      "                     131962  131963.0 2015-10-15 22:00:00     -57.491287   \n",
      "                     131980  131981.0 2015-10-16 16:00:00     -57.504365   \n",
      "\n",
      "                             location_lat individual_taxon_canonical_name  \\\n",
      "tag_local_identifier                                                        \n",
      "35957                131920    -16.889077                   Panthera onca   \n",
      "                     131927    -16.889124                   Panthera onca   \n",
      "                     131937    -16.888485                   Panthera onca   \n",
      "                     131962    -16.886804                   Panthera onca   \n",
      "                     131980    -16.879849                   Panthera onca   \n",
      "\n",
      "                             tag_local_identifier  \\\n",
      "tag_local_identifier                                \n",
      "35957                131920                     0   \n",
      "                     131927                     0   \n",
      "                     131937                     0   \n",
      "                     131962                     0   \n",
      "                     131980                     0   \n",
      "\n",
      "                             individual_local_identifier_ID     study_name  \\\n",
      "tag_local_identifier                                                         \n",
      "35957                131920                             117  Jaguar_Taiama   \n",
      "                     131927                             117  Jaguar_Taiama   \n",
      "                     131937                             117  Jaguar_Taiama   \n",
      "                     131962                             117  Jaguar_Taiama   \n",
      "                     131980                             117  Jaguar_Taiama   \n",
      "\n",
      "                            country  hour  ...  year  dayofweek        date  \\\n",
      "tag_local_identifier                       ...                                \n",
      "35957                131920  Brazil     3  ...  2015          2  2015-10-14   \n",
      "                     131927  Brazil    10  ...  2015          2  2015-10-14   \n",
      "                     131937  Brazil    21  ...  2015          2  2015-10-14   \n",
      "                     131962  Brazil    22  ...  2015          3  2015-10-15   \n",
      "                     131980  Brazil    16  ...  2015          4  2015-10-16   \n",
      "\n",
      "                             distance time_diff  velocity  direction  \\\n",
      "tag_local_identifier                                                   \n",
      "35957                131920     828.0      36.0    0.0064   2.793095   \n",
      "                     131927      12.0       7.0    0.0005   2.018546   \n",
      "                     131937     743.0      11.0    0.0188   1.479063   \n",
      "                     131962     199.0      25.0    0.0022  -0.361735   \n",
      "                     131980    1592.0      18.0    0.0246  -1.082039   \n",
      "\n",
      "                             movement  acceleration change_in_direction  \n",
      "tag_local_identifier                                                     \n",
      "35957                131920  movement       -0.0000              0.9880  \n",
      "                     131927  movement       -0.0008             -0.7745  \n",
      "                     131937  movement        0.0017             -0.5395  \n",
      "                     131962  movement       -0.0007             -1.8408  \n",
      "                     131980  movement        0.0012             -0.7203  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Data successfully loaded into SQLite database!\n",
      "=== DATA MERGED ===\n",
      "   year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
      "0  2016      7       16134  1.0   2.0   3.0  0.0   \n",
      "1  2003      5       47426  1.0   2.0   3.0  0.0   \n",
      "2  2018     10       89345  1.0   NaN   3.0  1.0   \n",
      "3  2020      1       92035  1.0   2.0   3.0  NaN   \n",
      "4  2014      6       94842  1.0   2.0   3.0  0.0   \n",
      "\n",
      "   monthly_mean_air_temperature  G1.3  sn.1  ...  Dgr  G4.7   iy    Gx    Gn  \\\n",
      "0                     56.670000   4.0   0.0  ...  1.0   NaN  NaN   NaN   NaN   \n",
      "1                     33.890000   4.0   0.0  ...  0.0   NaN  NaN   NaN   NaN   \n",
      "2                     95.560000   4.0   1.0  ...  NaN   NaN  NaN   NaN   NaN   \n",
      "3                     76.603193   4.0   0.0  ...  NaN   NaN  NaN   NaN   NaN   \n",
      "4                     50.000000   4.0   0.0  ...  1.0   7.0  1.0  23.0  23.0   \n",
      "\n",
      "               station_name                                      latitude  \\\n",
      "0              Monte Cimone                                         44.19   \n",
      "1                   Urakawa                                         42.16   \n",
      "2                Siple Dome                                        -81.65   \n",
      "3              Port Moresby                                        -09.44   \n",
      "4            Cape Otway/VIC                                        -38.85   \n",
      "\n",
      "    longitude   height                                            country  \n",
      "0       10.70     2173   Italy                                        ...  \n",
      "1      142.78       39   Japan                                        ...  \n",
      "2     -148.78      620   United States of America                     ...  \n",
      "3      147.22       48   Papua New Guinea                             ...  \n",
      "4      143.51       83   Australia                                    ...  \n",
      "\n",
      "[5 rows x 96 columns]\n",
      "Database connection closed.\n",
      "=== DATA MERGED COUNTRIES ===\n",
      "   event_id           timestamp  location_long  location_lat  \\\n",
      "0  131921.0 2015-10-14 03:00:00     -57.497704    -16.889077   \n",
      "1  131928.0 2015-10-14 10:00:00     -57.497605    -16.889124   \n",
      "2  131938.0 2015-10-14 21:00:00     -57.490651    -16.888485   \n",
      "3  131963.0 2015-10-15 22:00:00     -57.491287    -16.886804   \n",
      "4  131981.0 2015-10-16 16:00:00     -57.504365    -16.879849   \n",
      "\n",
      "  individual_taxon_canonical_name  tag_local_identifier  \\\n",
      "0                   Panthera onca                     0   \n",
      "1                   Panthera onca                     0   \n",
      "2                   Panthera onca                     0   \n",
      "3                   Panthera onca                     0   \n",
      "4                   Panthera onca                     0   \n",
      "\n",
      "   individual_local_identifier_ID     study_name country  hour  ...  velocity  \\\n",
      "0                             117  Jaguar_Taiama  Brazil     3  ...    0.0064   \n",
      "1                             117  Jaguar_Taiama  Brazil    10  ...    0.0005   \n",
      "2                             117  Jaguar_Taiama  Brazil    21  ...    0.0188   \n",
      "3                             117  Jaguar_Taiama  Brazil    22  ...    0.0022   \n",
      "4                             117  Jaguar_Taiama  Brazil    16  ...    0.0246   \n",
      "\n",
      "   direction  movement  acceleration change_in_direction  \\\n",
      "0   2.793095  movement       -0.0000              0.9880   \n",
      "1   2.018546  movement       -0.0008             -0.7745   \n",
      "2   1.479063  movement        0.0017             -0.5395   \n",
      "3  -0.361735  movement       -0.0007             -1.8408   \n",
      "4  -1.082039  movement        0.0012             -0.7203   \n",
      "\n",
      "   mean_daily_maximum_air_temperature  mean_daily_minimum_air_temperature  \\\n",
      "0                              147.22                               73.33   \n",
      "1                              147.22                               73.33   \n",
      "2                              147.22                               73.33   \n",
      "3                              147.22                               73.33   \n",
      "4                              147.22                               73.33   \n",
      "\n",
      "   total_precipitation_month  \\\n",
      "0                       23.0   \n",
      "1                       23.0   \n",
      "2                       23.0   \n",
      "3                       23.0   \n",
      "4                       23.0   \n",
      "\n",
      "   highest_daily_amount_precipitation_month_tenths_mm  \\\n",
      "0                                              218.0    \n",
      "1                                              218.0    \n",
      "2                                              218.0    \n",
      "3                                              218.0    \n",
      "4                                              218.0    \n",
      "\n",
      "  number_days_month_precipitation  \n",
      "0                             1.0  \n",
      "1                             1.0  \n",
      "2                             1.0  \n",
      "3                             1.0  \n",
      "4                             1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the reports and stations data by station_id\n",
    "import sqlite3\n",
    "\n",
    "data_merged = pd.merge(reports_data_renamed, stations_data_renamed, on='station_id')\n",
    "\n",
    "# data_fe['country'].unique()\n",
    "\n",
    "print(data_merged['highest_daily_amount_precipitation_month_tenths_mm'])\n",
    "\n",
    "data_merged['country'] = data_merged['country'].str.strip()\n",
    "# print(data_merged.columns)\n",
    "\n",
    "# print(data_merged['country'].unique())\n",
    "\n",
    "# data_merged['country'].isnull().sum()\n",
    "\n",
    "# print(data_merged.columns)\n",
    "\n",
    "month_daily_data = data_merged[['year', 'month', 'station_id', 'country', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature', 'total_precipitation_month', \n",
    "'number_days_month_precipitation', 'total_sunshine_month', 'percentage_total_sunshine_duration_relative_normal', 'days_month_maximum_air_temperature_25', \n",
    "'days_month_maximum_air_temperature_30', 'days_month_maximum_air_temperature_35', 'days_month_maximum_air_temperature_40', 'days_month_minimum_air_temperature_0', \n",
    "'days_month_maximum_air_temperature_0', 'days_month_precipitation_1', 'days_month_precipitation_5', 'days_month_precipitation_10', 'days_month_precipitation_50', \n",
    "'days_month_precipitation_100', 'days_month_precipitation_150', 'days_month_snow_depth_0', 'days_month_snow_depth_1', 'days_month_snow_depth_10', 'days_month_snow_depth_50', \n",
    "'days_month_visibility_50', 'days_month_visibility_100', 'days_month_visibility_1000', 'day_lowest_air_tempreature_month', 'day_highest_daily_mean_air_temperature_month', 'day_lowest_air_tempreature_month', \n",
    "'highest_daily_amount_precipitation_month_tenths_mm']]\n",
    "\n",
    "# print(data_fe.shape)\n",
    "\n",
    "# print(month_daily_data.head())\n",
    "\n",
    "# data_merged.head()\n",
    "\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "# Save data to SQLite database\n",
    "reports_data_renamed.to_sql('reports', conn, if_exists='replace', index=False)\n",
    "stations_data_renamed.to_sql('stations', conn, if_exists='replace', index=False)\n",
    "# data_merged.to_sql('reports_country', conn, if_exists='replace', index=False)\n",
    "data_fe.to_sql('data_fe', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"=== REPORTS DATA ===\")\n",
    "print(reports_data_renamed.head())\n",
    "\n",
    "print(\"=== STATIONS DATA ===\")\n",
    "print(stations_data_renamed.head())\n",
    "\n",
    "print(\"=== DATA FE ===\")\n",
    "print(data_fe.head())\n",
    "\n",
    "print(\"Data successfully loaded into SQLite database!\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Merge data_merged with data_fe only by the countries and date in data_fe\n",
    "# data_merged_countries = pd.merge(data_fe, month_daily_data, on=['country', 'year', 'month'], how='inner')\n",
    "\n",
    "# data_merged_countries = pd.merge(data_merged, data_fe, on='country', how='inner')\n",
    "# data_merged_countries = data_merged[data_merged['country'].isin(data_fe['country'].unique())]\n",
    "# data_merged_countries.head()\n",
    "\n",
    "# print(reports_data_renamed['highest_daily_amount_precipitation_month_tenths_mm'])\n",
    "\n",
    "# 1. Merge reports and stations\n",
    "data_merged = pd.merge(\n",
    "    reports_data_renamed,\n",
    "    stations_data_renamed,\n",
    "    on='station_id'\n",
    ")\n",
    "\n",
    "print(\"=== DATA MERGED ===\")\n",
    "print(data_merged.head())\n",
    "\n",
    "# print(data_merged['highest_daily_amount_precipitation_month_tenths_mm'])\n",
    "\n",
    "# 2. Clean and rename columns\n",
    "data_merged['country'] = data_merged['country'].str.strip()\n",
    "data_merged = data_merged.rename(columns={\n",
    "    'year_x': 'year',\n",
    "    'month_x': 'month'\n",
    "})\n",
    "\n",
    "# 3. Aggregate to country-month level\n",
    "country_month_climate = (\n",
    "    data_merged\n",
    "    .groupby(['country', 'year', 'month'])\n",
    "    .agg({\n",
    "        'mean_daily_maximum_air_temperature': 'mean',\n",
    "        'mean_daily_minimum_air_temperature': 'mean',\n",
    "        'total_precipitation_month': 'sum',\n",
    "        'highest_daily_amount_precipitation_month_tenths_mm': 'mean',\n",
    "        'number_days_month_precipitation': 'max'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "data_fe = data_fe.rename(columns={\n",
    "    'year_x': 'year',\n",
    "    'month_x': 'month'\n",
    "})\n",
    "\n",
    "# 4. Merge with movement data\n",
    "data_merged_countries = pd.merge(\n",
    "    data_fe,\n",
    "    country_month_climate,\n",
    "    on=['country', 'year', 'month'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Database connection closed.\")\n",
    "\n",
    "print(\"=== DATA MERGED COUNTRIES ===\")\n",
    "print(data_merged_countries.head())\n",
    "\n",
    "# Save\n",
    "data_merged_countries.to_csv('H:/Study/IPCA/MIAA/MLA/TP2025/DataS1/jaguar_movement_with_countries_climate_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
