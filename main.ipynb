{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from geopy.distance import geodesic\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12092\\3241155957.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location.long</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>individual.taxon.canonical.name</th>\n",
       "      <th>tag.local.identifier</th>\n",
       "      <th>individual.local.identifier (ID)</th>\n",
       "      <th>study.name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>2445.0</td>\n",
       "      <td>8/6/11 10:26</td>\n",
       "      <td>-58.064246</td>\n",
       "      <td>-23.251051</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>50886</td>\n",
       "      <td>7</td>\n",
       "      <td>Humid Chaco</td>\n",
       "      <td>Paraguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113991</th>\n",
       "      <td>113992.0</td>\n",
       "      <td>4/26/13 10:00</td>\n",
       "      <td>-64.822034</td>\n",
       "      <td>-3.033097</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>5</td>\n",
       "      <td>96</td>\n",
       "      <td>Jaguar_Mamiraua</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32852</th>\n",
       "      <td>32853.0</td>\n",
       "      <td>4/1/15 23:00</td>\n",
       "      <td>-57.448381</td>\n",
       "      <td>-16.857641</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>36318</td>\n",
       "      <td>22</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82777</th>\n",
       "      <td>82778.0</td>\n",
       "      <td>3/26/07 4:01</td>\n",
       "      <td>-60.274039</td>\n",
       "      <td>-20.566995</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>563517</td>\n",
       "      <td>76</td>\n",
       "      <td>Atlantic forest</td>\n",
       "      <td>Paraguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71176</th>\n",
       "      <td>71177.0</td>\n",
       "      <td>11/21/11 21:01</td>\n",
       "      <td>-56.278468</td>\n",
       "      <td>-20.081962</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>31935</td>\n",
       "      <td>68</td>\n",
       "      <td>jaguar_Oncafari Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Event_ID       timestamp  location.long  location.lat  \\\n",
       "2444      2445.0    8/6/11 10:26     -58.064246    -23.251051   \n",
       "113991  113992.0   4/26/13 10:00     -64.822034     -3.033097   \n",
       "32852    32853.0    4/1/15 23:00     -57.448381    -16.857641   \n",
       "82777    82778.0    3/26/07 4:01     -60.274039    -20.566995   \n",
       "71176    71177.0  11/21/11 21:01     -56.278468    -20.081962   \n",
       "\n",
       "       individual.taxon.canonical.name tag.local.identifier  \\\n",
       "2444                     Panthera onca                50886   \n",
       "113991                   Panthera onca                    5   \n",
       "32852                    Panthera onca                36318   \n",
       "82777                    Panthera onca               563517   \n",
       "71176                    Panthera onca                31935   \n",
       "\n",
       "        individual.local.identifier (ID)               study.name   country  \n",
       "2444                                   7              Humid Chaco  Paraguay  \n",
       "113991                                96          Jaguar_Mamiraua    Brazil  \n",
       "32852                                 22            Jaguar_Taiama    Brazil  \n",
       "82777                                 76          Atlantic forest  Paraguay  \n",
       "71176                                 68  jaguar_Oncafari Project    Brazil  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "data = data.sample(SAMPLE_SIZE) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event_ID                            0\n",
       "timestamp                           0\n",
       "location.long                       0\n",
       "location.lat                        0\n",
       "individual.taxon.canonical.name     0\n",
       "tag.local.identifier                0\n",
       "individual.local.identifier (ID)    0\n",
       "study.name                          0\n",
       "country                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cp = data.copy()\n",
    "data_cp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix features names Event_ID,timestamp,location.long,location.lat,individual.taxon.canonical.name,tag.local.identifier,individual.local.identifier (ID),study.name,country\n",
    "data_cp = data_cp.rename(columns={'Event_ID':'event_id','timestamp':'timestamp','location.long':'location_long','location.lat':'location_lat','individual.taxon.canonical.name':'individual_taxon_canonical_name','tag.local.identifier':'tag_local_identifier','individual.local.identifier (ID)':'individual_local_identifier_ID','study.name':'study_name','country':'country'})\n",
    "\n",
    "# Print features names\n",
    "data_cp.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12092\\3021486648.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "# Parsing the timestamp column into a datetime format\n",
    "data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n",
    "\n",
    "# Extract time of day, day of the week, month, etc., for temporal features.\n",
    "data_cp['hour'] = data_cp['timestamp'].dt.hour\n",
    "data_cp['day'] = data_cp['timestamp'].dt.day\n",
    "data_cp['month'] = data_cp['timestamp'].dt.month\n",
    "data_cp['year'] = data_cp['timestamp'].dt.year\n",
    "data_cp['dayofweek'] = data_cp['timestamp'].dt.dayofweek\n",
    "data_cp['date'] = data_cp['timestamp'].dt.date\n",
    "\n",
    "# Show unique values for day, month and year\n",
    "# print(data_cp['day'].unique())\n",
    "# print(data_cp['month'].unique())\n",
    "# print(data_cp['year'].unique())\n",
    "# print(data_cp['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country', 'hour',\n",
       "       'day', 'month', 'year', 'dayofweek', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cp = data_cp.drop(['event_id', 'individual_local_identifier_ID', 'tag_local_identifier'], axis=1)\n",
    "\n",
    "data_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances between consecutive events.\n",
    "### Calculating the distance between points using the Haversine Formula and group the data by tag_local_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_12092\\4228301756.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location_long</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>individual_taxon_canonical_name</th>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th>individual_local_identifier_ID</th>\n",
       "      <th>study_name</th>\n",
       "      <th>country</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131908</th>\n",
       "      <td>131909.0</td>\n",
       "      <td>2015-10-13 14:00:00</td>\n",
       "      <td>-57.500131</td>\n",
       "      <td>-16.883695</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>131918.0</td>\n",
       "      <td>2015-10-14 00:00:00</td>\n",
       "      <td>-57.498957</td>\n",
       "      <td>-16.885097</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131937</th>\n",
       "      <td>131938.0</td>\n",
       "      <td>2015-10-14 21:00:00</td>\n",
       "      <td>-57.490651</td>\n",
       "      <td>-16.888485</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131964</th>\n",
       "      <td>131965.0</td>\n",
       "      <td>2015-10-16 00:01:00</td>\n",
       "      <td>-57.491332</td>\n",
       "      <td>-16.886896</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131968</th>\n",
       "      <td>131969.0</td>\n",
       "      <td>2015-10-16 04:00:00</td>\n",
       "      <td>-57.491577</td>\n",
       "      <td>-16.885072</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event_id           timestamp  location_long  \\\n",
       "tag_local_identifier                                                       \n",
       "35957                131908  131909.0 2015-10-13 14:00:00     -57.500131   \n",
       "                     131917  131918.0 2015-10-14 00:00:00     -57.498957   \n",
       "                     131937  131938.0 2015-10-14 21:00:00     -57.490651   \n",
       "                     131964  131965.0 2015-10-16 00:01:00     -57.491332   \n",
       "                     131968  131969.0 2015-10-16 04:00:00     -57.491577   \n",
       "\n",
       "                             location_lat individual_taxon_canonical_name  \\\n",
       "tag_local_identifier                                                        \n",
       "35957                131908    -16.883695                   Panthera onca   \n",
       "                     131917    -16.885097                   Panthera onca   \n",
       "                     131937    -16.888485                   Panthera onca   \n",
       "                     131964    -16.886896                   Panthera onca   \n",
       "                     131968    -16.885072                   Panthera onca   \n",
       "\n",
       "                            tag_local_identifier  \\\n",
       "tag_local_identifier                               \n",
       "35957                131908                35957   \n",
       "                     131917                35957   \n",
       "                     131937                35957   \n",
       "                     131964                35957   \n",
       "                     131968                35957   \n",
       "\n",
       "                             individual_local_identifier_ID     study_name  \\\n",
       "tag_local_identifier                                                         \n",
       "35957                131908                             117  Jaguar_Taiama   \n",
       "                     131917                             117  Jaguar_Taiama   \n",
       "                     131937                             117  Jaguar_Taiama   \n",
       "                     131964                             117  Jaguar_Taiama   \n",
       "                     131968                             117  Jaguar_Taiama   \n",
       "\n",
       "                            country  hour  day  month  year  dayofweek  \\\n",
       "tag_local_identifier                                                     \n",
       "35957                131908  Brazil    14   13     10  2015          1   \n",
       "                     131917  Brazil     0   14     10  2015          2   \n",
       "                     131937  Brazil    21   14     10  2015          2   \n",
       "                     131964  Brazil     0   16     10  2015          4   \n",
       "                     131968  Brazil     4   16     10  2015          4   \n",
       "\n",
       "                                   date  distance  \n",
       "tag_local_identifier                               \n",
       "35957                131908  2015-10-13       NaN  \n",
       "                     131917  2015-10-14     0.200  \n",
       "                     131937  2015-10-14     0.961  \n",
       "                     131964  2015-10-16     0.191  \n",
       "                     131968  2015-10-16     0.204  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fe = data_cp.copy()\n",
    "\n",
    "data_fe['timestamp'] = pd.to_datetime(data_fe['timestamp'])\n",
    "\n",
    "# Define Haversine formula for distance calculation\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "# Function to calculate distance only for the same animal\n",
    "def calculate_distances_for_animal(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    group['distance'] = haversine(\n",
    "        group['location_lat'].shift(),\n",
    "        group['location_long'].shift(),\n",
    "        group['location_lat'],\n",
    "        group['location_long']\n",
    "    )\n",
    "    return group\n",
    "\n",
    "data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n",
    "\n",
    "# Convert distance to kilometers\n",
    "data_fe['distance'] = data_fe['distance'] / 1000\n",
    "data_fe['distance'] = data_fe['distance'].round(3)\n",
    "\n",
    "data_fe.head()\n",
    "# data_fe.to_csv('DataS1/jaguar_movement_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Labeling the Data\n",
    " Behavior Labeling:\n",
    " Manually label a subset of data with different behaviors (e.g., movement, hunting, resting).\n",
    " Consider cross-referencing domain knowledge or complementary data to infer labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movement > 0.5: 'hunting' (highest priority)\n",
    "\n",
    "0.1 < velocity ≤ 0.5: 'movement'\n",
    "\n",
    "0.0 < velocity ≤ 0.1: 'slowing down'\n",
    "\n",
    "≤ 0.0: 'resting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             velocity      movement\n",
      "tag_local_identifier                               \n",
      "35957                131917    0.0056  slowing down\n",
      "                     131937    0.0127  slowing down\n",
      "                     131964    0.0020  slowing down\n",
      "                     131968    0.0142  slowing down\n",
      "                     131998    0.0200  slowing down\n",
      "...                               ...           ...\n",
      "TIAGO                106008    0.0128  slowing down\n",
      "                     106035    0.0211  slowing down\n",
      "                     106109    0.0055  slowing down\n",
      "                     106142    0.0093  slowing down\n",
      "                     106144    0.0030  slowing down\n",
      "\n",
      "[4891 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat).\n",
    "# time_diff = (data_cp['timestamp'] - data_cp['timestamp'].shift()).dt.total_seconds()\n",
    "\n",
    "# Ensuring the distance is in meters and the time differece is in seconds\n",
    "time_diff = data_fe['time_diff'] = data_fe['timestamp'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "data_fe['distance'] = data_fe['distance'] * 1000\n",
    "time_diff_seconds = data_fe['time_diff'] * 3600\n",
    "\n",
    "# print(data_fe['time_diff'].head())\n",
    "\n",
    "data_fe['velocity'] = data_fe['distance'] / time_diff_seconds\n",
    "data_fe['velocity'] = data_fe['velocity'].round(4)\n",
    "data_fe = data_fe.dropna(subset=[ 'distance', 'velocity'])\n",
    "\n",
    "data_fe['direction'] = np.arctan2(data_fe['location_long'] - data_fe['location_long'].shift(), data_fe['location_lat'] - data_fe['location_lat'].shift())\n",
    "\n",
    "data_fe['movement'] = np.where(\n",
    "    data_fe['velocity'] > 0.5, 'hunting',\n",
    "    np.where(\n",
    "        data_fe['velocity'] > 0.1, 'movement',\n",
    "        np.where(\n",
    "            data_fe['velocity'] > 0.0, 'slowing down',\n",
    "            'resting'  # Velocity <= 0.0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# data_cp = data_cp.dropna()\n",
    "print(data_fe[['velocity', 'movement']])\n",
    "# print(data_fe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive acceleration or changes in movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acceleration</th>\n",
       "      <th>change_in_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131964</th>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-2.3628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131968</th>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131998</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131999</th>\n",
       "      <td>0.0011</td>\n",
       "      <td>-0.6324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132018</th>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.4251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             acceleration  change_in_direction\n",
       "tag_local_identifier                                          \n",
       "35957                131964       -0.0004              -2.3628\n",
       "                     131968        0.0031               0.2709\n",
       "                     131998        0.0002              -0.9435\n",
       "                     131999        0.0011              -0.6324\n",
       "                     132018       -0.0005               0.4251"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate acceleration as the change in velocity over time\n",
    "data_fe['acceleration'] = data_fe['velocity'].diff() / data_fe['time_diff']\n",
    "\n",
    "# Calculate change in direction\n",
    "data_fe['change_in_direction'] = data_fe['direction'].diff()\n",
    "\n",
    "# Drop rows with NaN values in acceleration and change_in_direction\n",
    "data_fe = data_fe.dropna(subset=['acceleration', 'change_in_direction'])\n",
    "\n",
    "# Round the values for better readability\n",
    "data_fe['acceleration'] = data_fe['acceleration'].round(4)\n",
    "data_fe['change_in_direction'] = data_fe['change_in_direction'].round(4)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "data_fe[['acceleration', 'change_in_direction']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n"
     ]
    }
   ],
   "source": [
    "# Encode tag_local_identifier to numerical values\n",
    "\n",
    "data_fe['tag_local_identifier'] = pd.Categorical(data_fe['tag_local_identifier'])\n",
    "data_fe['tag_local_identifier'] = data_fe['tag_local_identifier'].cat.codes\n",
    "\n",
    "print(data_fe['tag_local_identifier'].unique())\n",
    "# data_encoded = pd.get_dummies(data_fe, columns=['tag_local_identifier'])\n",
    "# data_encoded.columns\n",
    "# data_encoded['tag_local_identifier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to get temporal features from Paraguay, Brazil, Costa Rica, Argentina and Mexico between 1/1/1999 and 31/12/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.8)\n"
     ]
    }
   ],
   "source": [
    "# Get World  Climate data\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"christopherlemke/monthly-climat-reports-from-stations-worldwide\")\n",
    "\n",
    "reports_data = pd.read_csv(path + '/dwd-cdc_CLIMAT_reports_stations_ww.csv')\n",
    "stations_data = pd.read_csv(path + '/dwd-cdc_station_data_ww.csv')\n",
    "\n",
    "# Save the data to a csv file\n",
    "reports_data.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide.csv', index=False)\n",
    "stations_data.to_csv('DataS1/dwd-cdc_station_data_ww.csv', index=False)\n",
    "\n",
    "print(reports_data.shape)\n",
    "print(stations_data.shape)\n",
    "\n",
    "reports_data = reports_data.sample(SAMPLE_SIZE)\n",
    "# stations_data = stations_data.sample(SAMPLE_SIZE)\n",
    "\n",
    "# print(reports_data.head())\n",
    "# print(stations_data.head())\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Reports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farenheit_to_celsius(farenheit):\n",
    "    return (farenheit - 32) * 5.0/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Engineering\n",
    " Environmental Features:\n",
    " Integrate meteorological data (e.g., temperature, rainfall) if available.\n",
    " Integrate terrain data (e.g., elevation, land cover type) if available.\n",
    " Temporal Features:\n",
    " Extract time of day, day of week, and season from the timestamp.\n",
    " Identify specific behavioral periods like night vs. day.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly_mean_air_temperature          0\n",
      "mean_daily_maximum_air_temperature    0\n",
      "mean_daily_minimum_air_temperature    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>station_id</th>\n",
       "      <th>G1</th>\n",
       "      <th>G1.1</th>\n",
       "      <th>G1.2</th>\n",
       "      <th>sn</th>\n",
       "      <th>monthly_mean_air_temperature</th>\n",
       "      <th>G1.3</th>\n",
       "      <th>sn.1</th>\n",
       "      <th>...</th>\n",
       "      <th>iw</th>\n",
       "      <th>fx</th>\n",
       "      <th>yfx</th>\n",
       "      <th>G4.6</th>\n",
       "      <th>Dts</th>\n",
       "      <th>Dgr</th>\n",
       "      <th>G4.7</th>\n",
       "      <th>iy</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163212</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2935</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32780</th>\n",
       "      <td>2011</td>\n",
       "      <td>7</td>\n",
       "      <td>37621</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240560</th>\n",
       "      <td>2009</td>\n",
       "      <td>12</td>\n",
       "      <td>41254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196479</th>\n",
       "      <td>2009</td>\n",
       "      <td>11</td>\n",
       "      <td>17040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497416</th>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "      <td>41571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.89</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
       "163212  2014      1        2935  1.0   2.0   3.0  1.0   \n",
       "32780   2011      7       37621  1.0   2.0   3.0  0.0   \n",
       "240560  2009     12       41254  1.0   NaN   3.0  0.0   \n",
       "196479  2009     11       17040  1.0   2.0   3.0  0.0   \n",
       "497416  2006      7       41571  NaN   2.0   3.0  0.0   \n",
       "\n",
       "        monthly_mean_air_temperature  G1.3  sn.1  ...   iw     fx   yfx  G4.6  \\\n",
       "163212                         37.22   4.0   1.0  ...  1.0  110.0  10.0   NaN   \n",
       "32780                         126.67   4.0   0.0  ...  NaN    NaN   NaN   NaN   \n",
       "240560                         42.22   4.0   0.0  ...  3.0  354.0  14.0   6.0   \n",
       "196479                         50.00   4.0   0.0  ...  1.0  125.0  21.0   NaN   \n",
       "497416                        148.89   4.0   0.0  ...  4.0   33.0  13.0   NaN   \n",
       "\n",
       "        Dts  Dgr  G4.7   iy    Gx   Gn  \n",
       "163212  NaN  NaN   NaN  NaN   NaN  NaN  \n",
       "32780   NaN  NaN   NaN  NaN   NaN  NaN  \n",
       "240560  NaN  NaN   7.0  1.0  20.0  6.0  \n",
       "196479  NaN  NaN   NaN  NaN   NaN  NaN  \n",
       "497416  NaN  NaN   7.0  1.0  12.0  3.0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove columns\n",
    "reports_data_dropped = reports_data.drop(['Po', 'Po.1', 'P', 'P.1', 'T.1', 'st', 'st.1', \n",
    "                                          'Tx.1', 'Tn.1', \n",
    "                                          'e', 'e.1', \n",
    "                                          'R1.1', 'nr.1', 'S1.1', 'Rd', 'mp', 'mT', 'mTx', 'mTn', 'me', 'mR', 'mS', \n",
    "                                            #   'sn', 'sn.1', 'sn.2', 'sn.3', 'sn.4', 'sn.5', 'sn.6', 'sn.7', 'sn.8', 'sn.9',\n",
    "                                            # 'G1', 'G1.1', 'G1.2', 'G1.3', 'G1.4', 'G1.5', 'G1.6', 'G1.7', 'G1.8',\n",
    "                                            # 'G2', 'G2.1', 'G2.2', 'G2.3', 'G2.4', 'G2.5', 'G2.6', 'G2.7', 'G2.8', 'G2.9',\n",
    "                                            # 'G3', 'G3.1', 'G3.2', 'G3.3', 'G3.4', 'G3.5', 'G3.6', 'G3.7', 'G3.8', 'G3.9',\n",
    "                                            # 'G4', 'G4.1', 'G4.2', 'G4.3', 'G4.4', 'G4.5', 'G4.6', 'G4.7',\n",
    "                                            'Yb', 'Yc', 'P', 'YP', 'YR', 'YS', 'YT', 'YTx',\n",
    "                                            'Ye', 'G4', 'Txd', 'yx', 'Tnd', 'Tax', 'Tan' ], axis=1)\n",
    "\n",
    "# Drop columns with no data\n",
    "reports_data_dropped = reports_data_dropped.dropna(axis=1, how='all')\n",
    "\n",
    "# Rename columns\n",
    "reports_data_renamed = reports_data_dropped.rename(columns={'IIiii':'station_id', 'T':'monthly_mean_air_temperature', \n",
    "                                            'Tx':'mean_daily_maximum_air_temperature', \n",
    "                                            'Tn':'mean_daily_minimum_air_temperature', \n",
    "                                            'R1':'total_precipitation_month', 'S1':'total_sunshine_month', \n",
    "                                            'ps':'percentage_total_sunshine_duration_relative_normal', \n",
    "                                            'P0':'monthly_mean_pressure_station_level', 'e':'mean_vapor_pressure_month', \n",
    "                                            'nr':'number_days_month_precipitation', \n",
    "                                            'yP':'missing_years_air_pressure', 'yR':'missing_years_precipitation', 'yS':'missing_years_sunshine_duration', \n",
    "                                            'yT':'missing_years_mean_air_temperature', 'yTx':'missing_years_mean_extreme_air_temperature', 'ye':'missing_years_vapor_pressure', \n",
    "                                            'T25':'days_month_maximum_air_temperature_25', 'T30':'days_month_maximum_air_temperature_30', \n",
    "                                            'T35':'days_month_maximum_air_temperature_35', 'T40':'days_month_maximum_air_temperature_40', \n",
    "                                            'Tn0':'days_month_minimum_air_temperature_0', 'Tx0':'days_month_maximum_air_temperature_0', 'R01':'days_month_precipitation_1', \n",
    "                                            'R05':'days_month_precipitation_5', 'R10':'days_month_precipitation_10', 'R50':'days_month_precipitation_50', \n",
    "                                            'R100':'days_month_precipitation_100', 'R150':'days_month_precipitation_150', 's00':'days_month_snow_depth_0', \n",
    "                                            's01':'days_month_snow_depth_1', 's10':'days_month_snow_depth_10', 's50':'days_month_snow_depth_50', 'f10':'days_month_wind_speed_10', \n",
    "                                            'f20':'days_month_wind_speed_20', 'f30':'days_month_wind_speed_30', 'V1':'days_month_visibility_50', 'V2':'days_month_visibility_100', \n",
    "                                            'V3':'days_month_visibility_1000', 'yn': 'day_lowest_daily_mean_air_temperature_month', 'yax': 'day_highest_daily_mean_air_temperature_month', 'yan':'day_lowest_air_tempreature_month',\n",
    "                                            'Rx':'highest_daily_amount_precipitation_month_tenths_mm',  \n",
    "                                            # 'yr':'day_highest_daily_amount_precipitation_month'\n",
    "                                            })\n",
    "\n",
    "# Remove rows with missing values in [year]\n",
    "reports_data_renamed = reports_data_renamed.dropna(subset=['year'])\n",
    "\n",
    "# Enconde Year, Month, and Station ID to integer\n",
    "reports_data_renamed['year'] = reports_data_renamed['year'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['month'] = reports_data_renamed['month'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['station_id'] = reports_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['sn'] = reports_data_renamed['sn'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Convert temperature features to Celsius\n",
    "# °C = (°F - 32) × 5/9\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = farenheit_to_celsius(reports_data_renamed['monthly_mean_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] =  farenheit_to_celsius(reports_data_renamed['mean_daily_maximum_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = farenheit_to_celsius(reports_data_renamed['mean_daily_minimum_air_temperature']).round(2)\n",
    "\n",
    "# Check for missing values in temperature features\n",
    "# print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = reports_data_renamed['monthly_mean_air_temperature'].fillna(reports_data_renamed['monthly_mean_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] = reports_data_renamed['mean_daily_maximum_air_temperature'].fillna(reports_data_renamed['mean_daily_maximum_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = reports_data_renamed['mean_daily_minimum_air_temperature'].fillna(reports_data_renamed['mean_daily_minimum_air_temperature'].mean())\n",
    "\n",
    "print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# # Show null values\n",
    "# print(reports_data_renamed.isnull().sum())\n",
    "reports_data_renamed.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide-cleaned.csv', index=False)\n",
    "\n",
    "reports_data_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_data.dtypes\n",
    "\n",
    "# Rename columns\n",
    "# 0: Station ID, 1: Station Name, 2: Latitude, 3: Longitude, 4:Height, 5: Country\n",
    "stations_data_renamed = stations_data.rename(columns={'0':'station_id', '1':'station_name', '2':'latitude', '3':'longitude', '4':'height', '5':'country'})\n",
    "stations_data_renamed['station_id'] = stations_data_renamed['station_id'].str.strip()\n",
    "stations_data_renamed = stations_data_renamed[:-1]\n",
    "\n",
    "# print(stations_data_renamed['station_id'].unique())\n",
    "\n",
    "\n",
    "stations_data_renamed['station_id'] = (\n",
    "    stations_data_renamed['station_id']\n",
    "    .str.replace(r'\\D', '', regex=True)  # Remove all non-digit characters\n",
    "    .pipe(pd.to_numeric, errors='coerce')  # Convert to numeric, invalid become NaN\n",
    "    .astype('Int64')  # Convert to pandas' nullable Int64 type\n",
    ")\n",
    "\n",
    "\n",
    "# stations_data_renamed['station_id'] = stations_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "# stations_data_renamed.head()\n",
    "# stations_data_renamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded into SQLite database!\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Merge the reports and stations data by station_id\n",
    "import sqlite3\n",
    "\n",
    "data_merged = pd.merge(reports_data_renamed, stations_data_renamed, on='station_id')\n",
    "\n",
    "# data_fe['country'].unique()\n",
    "\n",
    "data_merged['country'] = data_merged['country'].str.strip()\n",
    "\n",
    "# print(data_merged['country'].unique())\n",
    "\n",
    "# data_merged['country'].isnull().sum()\n",
    "\n",
    "# print(data_merged.columns)\n",
    "\n",
    "month_daily_data = data_merged[['year', 'month', 'station_id', 'country', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature', 'total_precipitation_month', \n",
    "'number_days_month_precipitation', 'total_sunshine_month', 'percentage_total_sunshine_duration_relative_normal', 'days_month_maximum_air_temperature_25', \n",
    "'days_month_maximum_air_temperature_30', 'days_month_maximum_air_temperature_35', 'days_month_maximum_air_temperature_40', 'days_month_minimum_air_temperature_0', \n",
    "'days_month_maximum_air_temperature_0', 'days_month_precipitation_1', 'days_month_precipitation_5', 'days_month_precipitation_10', 'days_month_precipitation_50', \n",
    "'days_month_precipitation_100', 'days_month_precipitation_150', 'days_month_snow_depth_0', 'days_month_snow_depth_1', 'days_month_snow_depth_10', 'days_month_snow_depth_50', \n",
    "'days_month_visibility_50', 'days_month_visibility_100', 'days_month_visibility_1000', 'day_lowest_air_tempreature_month', 'day_highest_daily_mean_air_temperature_month', 'day_lowest_air_tempreature_month', 'highest_daily_amount_precipitation_month_tenths_mm']]\n",
    "\n",
    "# print(data_fe.shape)\n",
    "\n",
    "# print(month_daily_data.head())\n",
    "\n",
    "# data_merged.head()\n",
    "\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "# Save data to SQLite database\n",
    "reports_data_renamed.to_sql('reports', conn, if_exists='replace', index=False)\n",
    "stations_data_renamed.to_sql('stations', conn, if_exists='replace', index=False)\n",
    "# data_merged.to_sql('reports_country', conn, if_exists='replace', index=False)\n",
    "data_fe.to_sql('data_fe', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data successfully loaded into SQLite database!\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Merge data_merged with data_fe only by the countries in data_fe\n",
    "data_merged_countries = pd.merge(data_fe, month_daily_data, on='country')\n",
    "\n",
    "# data_merged_countries = pd.merge(data_merged, data_fe, on='country', how='inner')\n",
    "# data_merged_countries = data_merged[data_merged['country'].isin(data_fe['country'].unique())]\n",
    "# data_merged_countries.head()\n",
    "\n",
    "print(\"Database connection closed.\")\n",
    "\n",
    "# Save\n",
    "data_merged_countries.to_csv('DataS1/jaguar_movement_with_countries_climate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Selection\n",
    " Choose Algorithms:\n",
    " Evaluate models like Random Forest, Gradient Boosting Machines (e.g., XGBoost), or neural networks (RNN/LSTM for sequential data).\n",
    " Decide on the best model based on data type and problem complexity.\n",
    " Split Data:\n",
    " Split the data into training and testing sets (e.g., 80/20 or 70/30 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Data Preparation with Country Considerations\n",
    "\n",
    "# Defining target and features \n",
    "\n",
    "# Checking for leakage\n",
    "# Define features to EXCLUDE (potential leakage)\n",
    "leaky_features = [\n",
    "    'velocity', 'acceleration', 'distance', 'time_diff', 'direction',\n",
    "    'change_in_direction', 'movement'\n",
    "]\n",
    "\n",
    "# Safe features to include\n",
    "base_features = [\n",
    "    'location_long', 'location_lat', 'country', 'study_name',\n",
    "    'individual_taxon_canonical_name', 'hour', 'day', 'month_x', 'year_x',\n",
    "    'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature'\n",
    "]\n",
    "\n",
    "target = 'movement'\n",
    "# features = [col for col in data_fe.columns if col not in [target, 'event_id', 'timestamp', 'date']]\n",
    "features = [col for col in data_fe.columns if col in base_features]\n",
    "categorical_features = ['country', 'study_name', 'individual_taxon_canonical_name']\n",
    "numeric_features = [col for col in features if col not in categorical_features + [target]]\n",
    "data_fe['stratify_col'] = data_fe['country'] + '_' + data_fe[target]\n",
    "\n",
    "# Filter out classes with fewer than two samples\n",
    "class_counts = data_fe['stratify_col'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "data_fe_filtered = data_fe[data_fe['stratify_col'].isin(valid_classes)]\n",
    "\n",
    "data_fe_filtered = data_fe_filtered.reset_index(drop=True) \n",
    "\n",
    "# Splitting the data with stratification by both country and movement class\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_fe_filtered[features], \n",
    "    data_fe_filtered[target],\n",
    "    test_size=0.2,\n",
    "    stratify=data_fe_filtered['stratify_col'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# print(X_train.shape, X_test.shape)\n",
    "# print(X_train.columns, X_test.columns)\n",
    "\n",
    "# Model Selection\n",
    "models = {\n",
    "    # Handles categorical features natively, good for country-wise patterns\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        cat_features=categorical_features,\n",
    "        auto_class_weights='Balanced',\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    ),\n",
    "    \n",
    "    # Baseline for comparison\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Gradient boosting alternative\n",
    "    'XGBoost': XGBClassifier(\n",
    "        scale_pos_weight='balanced',\n",
    "        enable_categorical=False,  # Requires pre-encoded categories\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Training and Evaluation\n",
    " Train Models:\n",
    " Train selected models using the preprocessed data.\n",
    " Use cross-validation to tune model parameters.\n",
    " Evaluate Model:\n",
    " Evaluate performance using metrics like accuracy, precision, recall, and F1-score.\n",
    " If the dataset is imbalanced, apply techniques like SMOTE to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:848: UserWarning: The groups parameter is ignored by StratifiedKFold\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1-weighted: 0.999 ± 0.001\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    movement       0.98      1.00      0.99        45\n",
      "     resting       1.00      1.00      1.00         8\n",
      "slowing down       1.00      1.00      1.00       925\n",
      "\n",
      "    accuracy                           1.00       978\n",
      "   macro avg       0.99      1.00      1.00       978\n",
      "weighted avg       1.00      1.00      1.00       978\n",
      "\n",
      "\n",
      "=== Random Forest ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:848: UserWarning: The groups parameter is ignored by StratifiedKFold\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1-weighted: 0.997 ± 0.002\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    movement       1.00      1.00      1.00        45\n",
      "     resting       1.00      1.00      1.00         8\n",
      "slowing down       1.00      1.00      1.00       925\n",
      "\n",
      "    accuracy                           1.00       978\n",
      "   macro avg       1.00      1.00      1.00       978\n",
      "weighted avg       1.00      1.00      1.00       978\n",
      "\n",
      "\n",
      "=== XGBoost ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:848: UserWarning: The groups parameter is ignored by StratifiedKFold\n",
      "  warnings.warn(\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:49:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1-weighted: 1.000 ± 0.000\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    movement       1.00      1.00      1.00        45\n",
      "     resting       1.00      1.00      1.00         8\n",
      "slowing down       1.00      1.00      1.00       925\n",
      "\n",
      "    accuracy                           1.00       978\n",
      "   macro avg       1.00      1.00      1.00       978\n",
      "weighted avg       1.00      1.00      1.00       978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encoding target labels for non cat boost models\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    # Special handling for CatBoost\n",
    "    if name == 'CatBoost':\n",
    "        # Convert categorical columns to string type\n",
    "        X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "        X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "        \n",
    "        # Specify categorical features when fitting\n",
    "        model.fit(X_train, y_train, cat_features=categorical_features)\n",
    "    else:\n",
    "        # One-hot encode categorical features for other models\n",
    "        X_train_encoded = pd.get_dummies(X_train, columns=categorical_features)\n",
    "        X_test_encoded = pd.get_dummies(X_test, columns=categorical_features)\n",
    "        \n",
    "        # Align columns between train and test\n",
    "        X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "        \n",
    "        model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "    groups = data_fe_filtered.loc[X_train.index, 'country'].values\n",
    "    \n",
    "    # Cross-validate with regional stratification\n",
    "    cv_scores = cross_val_score(\n",
    "        model, \n",
    "        X_train if name == 'CatBoost' else X_train_encoded,\n",
    "        y_train if name == 'CatBoost' else y_train_encoded,\n",
    "        cv=5, \n",
    "        scoring='f1_weighted',\n",
    "        groups=groups\n",
    "    )\n",
    "\n",
    "    print(f\"CV F1-weighted: {np.mean(cv_scores):.3f} ± {np.std(cv_scores):.3f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if name == 'CatBoost':\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_encoded)\n",
    "    \n",
    "    if name != 'CatBoost':\n",
    "        y_pred = le.inverse_transform(y_pred)\n",
    "    \n",
    "    print(\"Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cross-Referencing with Complementary Data\n",
    " Integrate Meteorological Data:\n",
    " Merge weather data (e.g., temperature, humidity) with animal tracking data.\n",
    " Integrate Terrain Data:\n",
    " Merge terrain data (e.g., elevation, land use) with animal tracking data.\n",
    " Behavioral Biology Data:\n",
    " Incorporate knowledge from related behavioral biology studies (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Model Deployment\n",
    " Model Evaluation: Confirm that the model generalizes well to new or unseen data.\n",
    " Deploy Model:\n",
    " Prepare the model for deployment in a real-time or batch setting for wildlife tracking.\n",
    " Implement a user interface or tool to apply the model to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
