{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from geopy.distance import geodesic\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmars\\AppData\\Local\\Temp\\ipykernel_11772\\3241155957.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location.long</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>individual.taxon.canonical.name</th>\n",
       "      <th>tag.local.identifier</th>\n",
       "      <th>individual.local.identifier (ID)</th>\n",
       "      <th>study.name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23020</th>\n",
       "      <td>23021.0</td>\n",
       "      <td>11/29/11 22:02</td>\n",
       "      <td>-56.329004</td>\n",
       "      <td>-19.959032</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>31936</td>\n",
       "      <td>19</td>\n",
       "      <td>jaguar_Oncafari Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89025</th>\n",
       "      <td>89026.0</td>\n",
       "      <td>12/3/13 5:02</td>\n",
       "      <td>-57.491217</td>\n",
       "      <td>-16.860395</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>34770</td>\n",
       "      <td>81</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98722</th>\n",
       "      <td>98723.0</td>\n",
       "      <td>5/2/15 5:01</td>\n",
       "      <td>-57.384168</td>\n",
       "      <td>-16.986887</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>34770</td>\n",
       "      <td>81</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72098</th>\n",
       "      <td>72099.0</td>\n",
       "      <td>11/21/13 3:01</td>\n",
       "      <td>-56.263650</td>\n",
       "      <td>-19.881856</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>33301</td>\n",
       "      <td>69</td>\n",
       "      <td>jaguar_Oncafari Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82859</th>\n",
       "      <td>82860.0</td>\n",
       "      <td>4/14/07 4:03</td>\n",
       "      <td>-60.417902</td>\n",
       "      <td>-20.482090</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>563517</td>\n",
       "      <td>76</td>\n",
       "      <td>Atlantic forest</td>\n",
       "      <td>Paraguay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Event_ID       timestamp  location.long  location.lat  \\\n",
       "23020   23021.0  11/29/11 22:02     -56.329004    -19.959032   \n",
       "89025   89026.0    12/3/13 5:02     -57.491217    -16.860395   \n",
       "98722   98723.0     5/2/15 5:01     -57.384168    -16.986887   \n",
       "72098   72099.0   11/21/13 3:01     -56.263650    -19.881856   \n",
       "82859   82860.0    4/14/07 4:03     -60.417902    -20.482090   \n",
       "\n",
       "      individual.taxon.canonical.name tag.local.identifier  \\\n",
       "23020                   Panthera onca                31936   \n",
       "89025                   Panthera onca                34770   \n",
       "98722                   Panthera onca                34770   \n",
       "72098                   Panthera onca                33301   \n",
       "82859                   Panthera onca               563517   \n",
       "\n",
       "       individual.local.identifier (ID)               study.name   country  \n",
       "23020                                19  jaguar_Oncafari Project    Brazil  \n",
       "89025                                81            Jaguar_Taiama    Brazil  \n",
       "98722                                81            Jaguar_Taiama    Brazil  \n",
       "72098                                69  jaguar_Oncafari Project    Brazil  \n",
       "82859                                76          Atlantic forest  Paraguay  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "data = data.sample(SAMPLE_SIZE) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event_ID                            0\n",
       "timestamp                           0\n",
       "location.long                       0\n",
       "location.lat                        0\n",
       "individual.taxon.canonical.name     0\n",
       "tag.local.identifier                0\n",
       "individual.local.identifier (ID)    0\n",
       "study.name                          0\n",
       "country                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cp = data.copy()\n",
    "data_cp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix features names Event_ID,timestamp,location.long,location.lat,individual.taxon.canonical.name,tag.local.identifier,individual.local.identifier (ID),study.name,country\n",
    "data_cp = data_cp.rename(columns={'Event_ID':'event_id','timestamp':'timestamp','location.long':'location_long','location.lat':'location_lat','individual.taxon.canonical.name':'individual_taxon_canonical_name','tag.local.identifier':'tag_local_identifier','individual.local.identifier (ID)':'individual_local_identifier_ID','study.name':'study_name','country':'country'})\n",
    "\n",
    "# Print features names\n",
    "data_cp.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmars\\AppData\\Local\\Temp\\ipykernel_11772\\3021486648.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "# Parsing the timestamp column into a datetime format\n",
    "data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n",
    "\n",
    "# Extract time of day, day of the week, month, etc., for temporal features.\n",
    "data_cp['hour'] = data_cp['timestamp'].dt.hour\n",
    "data_cp['day'] = data_cp['timestamp'].dt.day\n",
    "data_cp['month'] = data_cp['timestamp'].dt.month\n",
    "data_cp['year'] = data_cp['timestamp'].dt.year\n",
    "data_cp['dayofweek'] = data_cp['timestamp'].dt.dayofweek\n",
    "data_cp['date'] = data_cp['timestamp'].dt.date\n",
    "\n",
    "# Show unique values for day, month and year\n",
    "# print(data_cp['day'].unique())\n",
    "# print(data_cp['month'].unique())\n",
    "# print(data_cp['year'].unique())\n",
    "# print(data_cp['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country', 'hour',\n",
       "       'day', 'month', 'year', 'dayofweek', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cp = data_cp.drop(['event_id', 'individual_local_identifier_ID', 'tag_local_identifier'], axis=1)\n",
    "\n",
    "data_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances between consecutive events.\n",
    "### Calculating the distance between points using the Haversine Formula and group the data by tag_local_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location_long</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>individual_taxon_canonical_name</th>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th>individual_local_identifier_ID</th>\n",
       "      <th>study_name</th>\n",
       "      <th>country</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131878</th>\n",
       "      <td>131879.0</td>\n",
       "      <td>2015-10-12 08:00:00</td>\n",
       "      <td>-57.502841</td>\n",
       "      <td>-16.880885</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131901</th>\n",
       "      <td>131902.0</td>\n",
       "      <td>2015-10-13 07:00:00</td>\n",
       "      <td>-57.500805</td>\n",
       "      <td>-16.883557</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-13</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>131918.0</td>\n",
       "      <td>2015-10-14 00:00:00</td>\n",
       "      <td>-57.498957</td>\n",
       "      <td>-16.885097</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131926</th>\n",
       "      <td>131927.0</td>\n",
       "      <td>2015-10-14 09:00:00</td>\n",
       "      <td>-57.497686</td>\n",
       "      <td>-16.889175</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131931</th>\n",
       "      <td>131932.0</td>\n",
       "      <td>2015-10-14 14:00:00</td>\n",
       "      <td>-57.497668</td>\n",
       "      <td>-16.889110</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event_id           timestamp  location_long  \\\n",
       "tag_local_identifier                                                       \n",
       "35957                131878  131879.0 2015-10-12 08:00:00     -57.502841   \n",
       "                     131901  131902.0 2015-10-13 07:00:00     -57.500805   \n",
       "                     131917  131918.0 2015-10-14 00:00:00     -57.498957   \n",
       "                     131926  131927.0 2015-10-14 09:00:00     -57.497686   \n",
       "                     131931  131932.0 2015-10-14 14:00:00     -57.497668   \n",
       "\n",
       "                             location_lat individual_taxon_canonical_name  \\\n",
       "tag_local_identifier                                                        \n",
       "35957                131878    -16.880885                   Panthera onca   \n",
       "                     131901    -16.883557                   Panthera onca   \n",
       "                     131917    -16.885097                   Panthera onca   \n",
       "                     131926    -16.889175                   Panthera onca   \n",
       "                     131931    -16.889110                   Panthera onca   \n",
       "\n",
       "                            tag_local_identifier  \\\n",
       "tag_local_identifier                               \n",
       "35957                131878                35957   \n",
       "                     131901                35957   \n",
       "                     131917                35957   \n",
       "                     131926                35957   \n",
       "                     131931                35957   \n",
       "\n",
       "                             individual_local_identifier_ID     study_name  \\\n",
       "tag_local_identifier                                                         \n",
       "35957                131878                             117  Jaguar_Taiama   \n",
       "                     131901                             117  Jaguar_Taiama   \n",
       "                     131917                             117  Jaguar_Taiama   \n",
       "                     131926                             117  Jaguar_Taiama   \n",
       "                     131931                             117  Jaguar_Taiama   \n",
       "\n",
       "                            country  hour  day  month  year  dayofweek  \\\n",
       "tag_local_identifier                                                     \n",
       "35957                131878  Brazil     8   12     10  2015          0   \n",
       "                     131901  Brazil     7   13     10  2015          1   \n",
       "                     131917  Brazil     0   14     10  2015          2   \n",
       "                     131926  Brazil     9   14     10  2015          2   \n",
       "                     131931  Brazil    14   14     10  2015          2   \n",
       "\n",
       "                                   date  distance  \n",
       "tag_local_identifier                               \n",
       "35957                131878  2015-10-12       NaN  \n",
       "                     131901  2015-10-13     0.368  \n",
       "                     131917  2015-10-14     0.261  \n",
       "                     131926  2015-10-14     0.473  \n",
       "                     131931  2015-10-14     0.007  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fe = data_cp.copy()\n",
    "\n",
    "data_fe['timestamp'] = pd.to_datetime(data_fe['timestamp'])\n",
    "\n",
    "# Define Haversine formula for distance calculation\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "# Function to calculate distance only for the same animal\n",
    "def calculate_distances_for_animal(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    group['distance'] = haversine(\n",
    "        group['location_lat'].shift(),\n",
    "        group['location_long'].shift(),\n",
    "        group['location_lat'],\n",
    "        group['location_long']\n",
    "    )\n",
    "    return group\n",
    "\n",
    "data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n",
    "\n",
    "# Convert distance to kilometers\n",
    "data_fe['distance'] = data_fe['distance'] / 1000\n",
    "data_fe['distance'] = data_fe['distance'].round(3)\n",
    "\n",
    "data_fe.head()\n",
    "# data_fe.to_csv('DataS1/jaguar_movement_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Labeling the Data\n",
    " Behavior Labeling:\n",
    " Manually label a subset of data with different behaviors (e.g., movement, hunting, resting).\n",
    " Consider cross-referencing domain knowledge or complementary data to infer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             velocity  movement\n",
      "tag_local_identifier                           \n",
      "35957                131901    0.0044  movement\n",
      "                     131917    0.0043  movement\n",
      "                     131926    0.0146  movement\n",
      "                     131931    0.0004  movement\n",
      "                     131956    0.0078  movement\n",
      "...                               ...       ...\n",
      "TIAGO                105877    0.0144  movement\n",
      "                     105967    0.0018  movement\n",
      "                     105991    0.0197  movement\n",
      "                     106108    0.0127  movement\n",
      "                     106112    0.0027  movement\n",
      "\n",
      "[4889 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat).\n",
    "# time_diff = (data_cp['timestamp'] - data_cp['timestamp'].shift()).dt.total_seconds()\n",
    "\n",
    "# Ensuring the distance is in meters and the time differece is in seconds\n",
    "time_diff = data_fe['time_diff'] = data_fe['timestamp'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "data_fe['distance'] = data_fe['distance'] * 1000\n",
    "time_diff_seconds = data_fe['time_diff'] * 3600\n",
    "\n",
    "# print(data_fe['time_diff'].head())\n",
    "\n",
    "data_fe['velocity'] = data_fe['distance'] / time_diff_seconds\n",
    "data_fe['velocity'] = data_fe['velocity'].round(4)\n",
    "data_fe = data_fe.dropna(subset=[ 'distance', 'velocity'])\n",
    "\n",
    "data_fe['direction'] = np.arctan2(data_fe['location_long'] - data_fe['location_long'].shift(), data_fe['location_lat'] - data_fe['location_lat'].shift())\n",
    "\n",
    "# movement, hunting, resting, or slowing down\n",
    "data_fe['movement'] = np.where(data_fe['velocity'] > 0.1, 'movement', 'resting')\n",
    "data_fe['movement'] = np.where(data_fe['velocity'] > 0.5, 'movement', 'hunting')\n",
    "data_fe['movement'] = np.where(data_fe['velocity'] < 0.1, 'movement', 'slowing down')\n",
    "\n",
    "# data_cp = data_cp.dropna()\n",
    "print(data_fe[['velocity', 'movement']])\n",
    "# print(data_fe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive acceleration or changes in movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acceleration</th>\n",
       "      <th>change_in_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131926</th>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.5739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131931</th>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-2.5793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131956</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131963</th>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-2.4232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131972</th>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.9281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             acceleration  change_in_direction\n",
       "tag_local_identifier                                          \n",
       "35957                131926        0.0011               0.5739\n",
       "                     131931       -0.0028              -2.5793\n",
       "                     131956        0.0003               0.9889\n",
       "                     131963       -0.0010              -2.4232\n",
       "                     131972        0.0007               0.9281"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate acceleration as the change in velocity over time\n",
    "data_fe['acceleration'] = data_fe['velocity'].diff() / data_fe['time_diff']\n",
    "\n",
    "# Calculate change in direction\n",
    "data_fe['change_in_direction'] = data_fe['direction'].diff()\n",
    "\n",
    "# Drop rows with NaN values in acceleration and change_in_direction\n",
    "data_fe = data_fe.dropna(subset=['acceleration', 'change_in_direction'])\n",
    "\n",
    "# Round the values for better readability\n",
    "data_fe['acceleration'] = data_fe['acceleration'].round(4)\n",
    "data_fe['change_in_direction'] = data_fe['change_in_direction'].round(4)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "data_fe[['acceleration', 'change_in_direction']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n"
     ]
    }
   ],
   "source": [
    "# Encode tag_local_identifier to numerical values\n",
    "\n",
    "data_fe['tag_local_identifier'] = pd.Categorical(data_fe['tag_local_identifier'])\n",
    "data_fe['tag_local_identifier'] = data_fe['tag_local_identifier'].cat.codes\n",
    "\n",
    "print(data_fe['tag_local_identifier'].unique())\n",
    "# data_encoded = pd.get_dummies(data_fe, columns=['tag_local_identifier'])\n",
    "# data_encoded.columns\n",
    "# data_encoded['tag_local_identifier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to get temporal features from Paraguay, Brazil, Costa Rica, Argentina and Mexico between 1/1/1999 and 31/12/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.7).\n",
      "(553652, 127)\n",
      "(4169, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get World  Climate data\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"christopherlemke/monthly-climat-reports-from-stations-worldwide\")\n",
    "\n",
    "reports_data = pd.read_csv(path + '/dwd-cdc_CLIMAT_reports_stations_ww.csv')\n",
    "stations_data = pd.read_csv(path + '/dwd-cdc_station_data_ww.csv')\n",
    "\n",
    "# Save the data to a csv file\n",
    "reports_data.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide.csv', index=False)\n",
    "stations_data.to_csv('DataS1/dwd-cdc_station_data_ww.csv', index=False)\n",
    "\n",
    "print(reports_data.shape)\n",
    "print(stations_data.shape)\n",
    "\n",
    "reports_data = reports_data.sample(SAMPLE_SIZE)\n",
    "# stations_data = stations_data.sample(SAMPLE_SIZE)\n",
    "\n",
    "# print(reports_data.head())\n",
    "# print(stations_data.head())\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Reports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farenheit_to_celsius(farenheit):\n",
    "    return (farenheit - 32) * 5.0/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Engineering\n",
    " Environmental Features:\n",
    " Integrate meteorological data (e.g., temperature, rainfall) if available.\n",
    " Integrate terrain data (e.g., elevation, land cover type) if available.\n",
    " Temporal Features:\n",
    " Extract time of day, day of week, and season from the timestamp.\n",
    " Identify specific behavioral periods like night vs. day.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly_mean_air_temperature          0\n",
      "mean_daily_maximum_air_temperature    0\n",
      "mean_daily_minimum_air_temperature    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>station_id</th>\n",
       "      <th>G1</th>\n",
       "      <th>G1.1</th>\n",
       "      <th>G1.2</th>\n",
       "      <th>sn</th>\n",
       "      <th>monthly_mean_air_temperature</th>\n",
       "      <th>G1.3</th>\n",
       "      <th>sn.1</th>\n",
       "      <th>...</th>\n",
       "      <th>iw</th>\n",
       "      <th>fx</th>\n",
       "      <th>yfx</th>\n",
       "      <th>G4.6</th>\n",
       "      <th>Dts</th>\n",
       "      <th>Dgr</th>\n",
       "      <th>G4.7</th>\n",
       "      <th>iy</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27189</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>28434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.44</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131197</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>30054</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172314</th>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>43041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294283</th>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>38611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388019</th>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>38656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
       "27189   2008     12       28434  1.0   2.0   3.0  1.0   \n",
       "131197  2017     11       30054  1.0   2.0   3.0  1.0   \n",
       "172314  2010     11       43041  1.0   2.0   3.0  0.0   \n",
       "294283  2016     12       38611  1.0   2.0   3.0  0.0   \n",
       "388019  2012      8       38656  1.0   2.0   3.0  0.0   \n",
       "\n",
       "        monthly_mean_air_temperature  G1.3  sn.1  ...  iw  fx  yfx  G4.6  Dts  \\\n",
       "27189                          39.44   4.0   1.0  ... NaN NaN  NaN   NaN  NaN   \n",
       "131197                         85.00   4.0   1.0  ... NaN NaN  NaN   NaN  NaN   \n",
       "172314                        116.67   4.0   0.0  ... NaN NaN  NaN   NaN  NaN   \n",
       "294283                         -2.22   4.0   0.0  ... NaN NaN  NaN   NaN  NaN   \n",
       "388019                        158.33   4.0   0.0  ... NaN NaN  NaN   NaN  NaN   \n",
       "\n",
       "        Dgr  G4.7  iy  Gx  Gn  \n",
       "27189   NaN   NaN NaN NaN NaN  \n",
       "131197  NaN   NaN NaN NaN NaN  \n",
       "172314  NaN   NaN NaN NaN NaN  \n",
       "294283  NaN   NaN NaN NaN NaN  \n",
       "388019  NaN   NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove columns\n",
    "reports_data_dropped = reports_data.drop(['Po', 'Po.1', 'P', 'P.1', 'T.1', 'st', 'st.1', \n",
    "                                          'Tx.1', 'Tn.1', \n",
    "                                          'e', 'e.1', \n",
    "                                          'R1.1', 'nr.1', 'S1.1', 'Rd', 'mp', 'mT', 'mTx', 'mTn', 'me', 'mR', 'mS', \n",
    "                                            #   'sn', 'sn.1', 'sn.2', 'sn.3', 'sn.4', 'sn.5', 'sn.6', 'sn.7', 'sn.8', 'sn.9',\n",
    "                                            # 'G1', 'G1.1', 'G1.2', 'G1.3', 'G1.4', 'G1.5', 'G1.6', 'G1.7', 'G1.8',\n",
    "                                            # 'G2', 'G2.1', 'G2.2', 'G2.3', 'G2.4', 'G2.5', 'G2.6', 'G2.7', 'G2.8', 'G2.9',\n",
    "                                            # 'G3', 'G3.1', 'G3.2', 'G3.3', 'G3.4', 'G3.5', 'G3.6', 'G3.7', 'G3.8', 'G3.9',\n",
    "                                            # 'G4', 'G4.1', 'G4.2', 'G4.3', 'G4.4', 'G4.5', 'G4.6', 'G4.7',\n",
    "                                            'Yb', 'Yc', 'P', 'YP', 'YR', 'YS', 'YT', 'YTx',\n",
    "                                            'Ye', 'G4', 'Txd', 'yx', 'Tnd', 'Tax', 'Tan' ], axis=1)\n",
    "\n",
    "# Drop columns with no data\n",
    "reports_data_dropped = reports_data_dropped.dropna(axis=1, how='all')\n",
    "\n",
    "# Rename columns\n",
    "reports_data_renamed = reports_data_dropped.rename(columns={'IIiii':'station_id', 'T':'monthly_mean_air_temperature', \n",
    "                                            'Tx':'mean_daily_maximum_air_temperature', \n",
    "                                            'Tn':'mean_daily_minimum_air_temperature', \n",
    "                                            'R1':'total_precipitation_month', 'S1':'total_sunshine_month', \n",
    "                                            'ps':'percentage_total_sunshine_duration_relative_normal', \n",
    "                                            'P0':'monthly_mean_pressure_station_level', 'e':'mean_vapor_pressure_month', \n",
    "                                            'nr':'number_days_month_precipitation', \n",
    "                                            'yP':'missing_years_air_pressure', 'yR':'missing_years_precipitation', 'yS':'missing_years_sunshine_duration', \n",
    "                                            'yT':'missing_years_mean_air_temperature', 'yTx':'missing_years_mean_extreme_air_temperature', 'ye':'missing_years_vapor_pressure', \n",
    "                                            'T25':'days_month_maximum_air_temperature_25', 'T30':'days_month_maximum_air_temperature_30', \n",
    "                                            'T35':'days_month_maximum_air_temperature_35', 'T40':'days_month_maximum_air_temperature_40', \n",
    "                                            'Tn0':'days_month_minimum_air_temperature_0', 'Tx0':'days_month_maximum_air_temperature_0', 'R01':'days_month_precipitation_1', \n",
    "                                            'R05':'days_month_precipitation_5', 'R10':'days_month_precipitation_10', 'R50':'days_month_precipitation_50', \n",
    "                                            'R100':'days_month_precipitation_100', 'R150':'days_month_precipitation_150', 's00':'days_month_snow_depth_0', \n",
    "                                            's01':'days_month_snow_depth_1', 's10':'days_month_snow_depth_10', 's50':'days_month_snow_depth_50', 'f10':'days_month_wind_speed_10', \n",
    "                                            'f20':'days_month_wind_speed_20', 'f30':'days_month_wind_speed_30', 'V1':'days_month_visibility_50', 'V2':'days_month_visibility_100', \n",
    "                                            'V3':'days_month_visibility_1000', 'yn': 'day_lowest_daily_mean_air_temperature_month', 'yax': 'day_highest_daily_mean_air_temperature_month', 'yan':'day_lowest_air_tempreature_month',\n",
    "                                            'Rx':'highest_daily_amount_precipitation_month_tenths_mm',  \n",
    "                                            # 'yr':'day_highest_daily_amount_precipitation_month'\n",
    "                                            })\n",
    "\n",
    "# Remove rows with missing values in [year]\n",
    "reports_data_renamed = reports_data_renamed.dropna(subset=['year'])\n",
    "\n",
    "# Enconde Year, Month, and Station ID to integer\n",
    "reports_data_renamed['year'] = reports_data_renamed['year'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['month'] = reports_data_renamed['month'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['station_id'] = reports_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['sn'] = reports_data_renamed['sn'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Convert temperature features to Celsius\n",
    "# °C = (°F - 32) × 5/9\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = farenheit_to_celsius(reports_data_renamed['monthly_mean_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] =  farenheit_to_celsius(reports_data_renamed['mean_daily_maximum_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = farenheit_to_celsius(reports_data_renamed['mean_daily_minimum_air_temperature']).round(2)\n",
    "\n",
    "# Check for missing values in temperature features\n",
    "# print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = reports_data_renamed['monthly_mean_air_temperature'].fillna(reports_data_renamed['monthly_mean_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] = reports_data_renamed['mean_daily_maximum_air_temperature'].fillna(reports_data_renamed['mean_daily_maximum_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = reports_data_renamed['mean_daily_minimum_air_temperature'].fillna(reports_data_renamed['mean_daily_minimum_air_temperature'].mean())\n",
    "\n",
    "print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# # Show null values\n",
    "# print(reports_data_renamed.isnull().sum())\n",
    "reports_data_renamed.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide-cleaned.csv', index=False)\n",
    "\n",
    "reports_data_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_data.dtypes\n",
    "\n",
    "# Rename columns\n",
    "# 0: Station ID, 1: Station Name, 2: Latitude, 3: Longitude, 4:Height, 5: Country\n",
    "stations_data_renamed = stations_data.rename(columns={'0':'station_id', '1':'station_name', '2':'latitude', '3':'longitude', '4':'height', '5':'country'})\n",
    "stations_data_renamed['station_id'] = stations_data_renamed['station_id'].str.strip()\n",
    "stations_data_renamed = stations_data_renamed[:-1]\n",
    "\n",
    "# print(stations_data_renamed['station_id'].unique())\n",
    "\n",
    "\n",
    "stations_data_renamed['station_id'] = (\n",
    "    stations_data_renamed['station_id']\n",
    "    .str.replace(r'\\D', '', regex=True)  # Remove all non-digit characters\n",
    "    .pipe(pd.to_numeric, errors='coerce')  # Convert to numeric, invalid become NaN\n",
    "    .astype('Int64')  # Convert to pandas' nullable Int64 type\n",
    ")\n",
    "\n",
    "\n",
    "# stations_data_renamed['station_id'] = stations_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "# stations_data_renamed.head()\n",
    "# stations_data_renamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4673, 96)\n",
      "(4887, 22)\n",
      "Data successfully loaded into SQLite database!\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Merge the reports and stations data by station_id\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "data_merged = pd.merge(reports_data_renamed, stations_data_renamed, on='station_id')\n",
    "\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "# data_fe['country'].unique()\n",
    "data_merged['country'] = data_merged['country'].str.strip()\n",
    "data_merged['country'].unique()\n",
    "# data_merged['country'].isnull().sum()\n",
    "\n",
    "print(data_merged.shape)\n",
    "print(data_fe.shape)\n",
    "# data_merged.head()\n",
    "\n",
    "# Save data to SQLite database\n",
    "reports_data_renamed.to_sql('reports', conn, if_exists='replace', index=False)\n",
    "stations_data_renamed.to_sql('stations', conn, if_exists='replace', index=False)\n",
    "data_fe.to_sql('data_fe', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data successfully loaded into SQLite database!\")\n",
    "\n",
    "# Merge data_merged with data_fe only by the countries in data_fe\n",
    "data_merged_countries = pd.merge(data_fe, data_merged, on='country')\n",
    "\n",
    "# data_merged_countries = pd.merge(data_merged, data_fe, on='country', how='inner')\n",
    "# data_merged_countries = data_merged[data_merged['country'].isin(data_fe['country'].unique())]\n",
    "# data_merged_countries.head()\n",
    "\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n",
    "\n",
    "# Save\n",
    "data_merged_countries.to_csv('DataS1/jaguar_movement_with_countries_climate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prec = pd.read_csv('DataS1/average-precipitation-per-year.csv')\n",
    "avg_prec.drop(['Code'], axis=1, inplace=True)\n",
    "avg_prec.rename(columns={'Entity':'country', 'Year':'year'}, inplace=True)\n",
    "avg_prec.head()\n",
    "\n",
    "# Merge the two datasets by Country\n",
    "data_merged = pd.merge(data_fe, avg_prec, on=['country', 'year'], how='left')\n",
    "data_merged.head()\n",
    "\n",
    "# Save\n",
    "data_merged.to_csv('DataS1/jaguar_movement_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Selection\n",
    " Choose Algorithms:\n",
    " Evaluate models like Random Forest, Gradient Boosting Machines (e.g., XGBoost), or neural networks (RNN/LSTM for sequential data).\n",
    " Decide on the best model based on data type and problem complexity.\n",
    " Split Data:\n",
    " Split the data into training and testing sets (e.g., 80/20 or 70/30 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Training and Evaluation\n",
    " Train Models:\n",
    " Train selected models using the preprocessed data.\n",
    " Use cross-validation to tune model parameters.\n",
    " Evaluate Model:\n",
    " Evaluate performance using metrics like accuracy, precision, recall, and F1-score.\n",
    " If the dataset is imbalanced, apply techniques like SMOTE to balance it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cross-Referencing with Complementary Data\n",
    " Integrate Meteorological Data:\n",
    " Merge weather data (e.g., temperature, humidity) with animal tracking data.\n",
    " Integrate Terrain Data:\n",
    " Merge terrain data (e.g., elevation, land use) with animal tracking data.\n",
    " Behavioral Biology Data:\n",
    " Incorporate knowledge from related behavioral biology studies (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Model Deployment\n",
    " Model Evaluation: Confirm that the model generalizes well to new or unseen data.\n",
    " Deploy Model:\n",
    " Prepare the model for deployment in a real-time or batch setting for wildlife tracking.\n",
    " Implement a user interface or tool to apply the model to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
