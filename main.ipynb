{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from geopy.distance import geodesic\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_17528\\3241155957.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location.long</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>individual.taxon.canonical.name</th>\n",
       "      <th>tag.local.identifier</th>\n",
       "      <th>individual.local.identifier (ID)</th>\n",
       "      <th>study.name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15584</th>\n",
       "      <td>15585.0</td>\n",
       "      <td>11/20/13 11:00</td>\n",
       "      <td>-56.319152</td>\n",
       "      <td>-19.932593</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>33299</td>\n",
       "      <td>15</td>\n",
       "      <td>jaguar_Oncafari Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98275</th>\n",
       "      <td>98276.0</td>\n",
       "      <td>3/30/15 4:01</td>\n",
       "      <td>-57.380348</td>\n",
       "      <td>-16.998639</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>34770</td>\n",
       "      <td>81</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21923</th>\n",
       "      <td>21924.0</td>\n",
       "      <td>2/28/15 19:00</td>\n",
       "      <td>-57.418636</td>\n",
       "      <td>-16.958635</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>36317</td>\n",
       "      <td>18</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123637</th>\n",
       "      <td>123638.0</td>\n",
       "      <td>10/14/08 23:00</td>\n",
       "      <td>-56.895966</td>\n",
       "      <td>-19.462106</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>152200</td>\n",
       "      <td>111</td>\n",
       "      <td>Sao Bento</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>36470.0</td>\n",
       "      <td>7/28/15 13:00</td>\n",
       "      <td>-56.274572</td>\n",
       "      <td>-19.904187</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>36112</td>\n",
       "      <td>25</td>\n",
       "      <td>jaguar_Oncafari Project</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Event_ID       timestamp  location.long  location.lat  \\\n",
       "15584    15585.0  11/20/13 11:00     -56.319152    -19.932593   \n",
       "98275    98276.0    3/30/15 4:01     -57.380348    -16.998639   \n",
       "21923    21924.0   2/28/15 19:00     -57.418636    -16.958635   \n",
       "123637  123638.0  10/14/08 23:00     -56.895966    -19.462106   \n",
       "36469    36470.0   7/28/15 13:00     -56.274572    -19.904187   \n",
       "\n",
       "       individual.taxon.canonical.name tag.local.identifier  \\\n",
       "15584                    Panthera onca                33299   \n",
       "98275                    Panthera onca                34770   \n",
       "21923                    Panthera onca                36317   \n",
       "123637                   Panthera onca               152200   \n",
       "36469                    Panthera onca                36112   \n",
       "\n",
       "        individual.local.identifier (ID)               study.name country  \n",
       "15584                                 15  jaguar_Oncafari Project  Brazil  \n",
       "98275                                 81            Jaguar_Taiama  Brazil  \n",
       "21923                                 18            Jaguar_Taiama  Brazil  \n",
       "123637                               111                Sao Bento  Brazil  \n",
       "36469                                 25  jaguar_Oncafari Project  Brazil  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('DataS1/jaguar_movement_data.csv')\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "data = data.sample(SAMPLE_SIZE) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event_ID                            0\n",
       "timestamp                           0\n",
       "location.long                       0\n",
       "location.lat                        0\n",
       "individual.taxon.canonical.name     0\n",
       "tag.local.identifier                0\n",
       "individual.local.identifier (ID)    0\n",
       "study.name                          0\n",
       "country                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cp = data.copy()\n",
    "data_cp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix features names Event_ID,timestamp,location.long,location.lat,individual.taxon.canonical.name,tag.local.identifier,individual.local.identifier (ID),study.name,country\n",
    "data_cp = data_cp.rename(columns={'Event_ID':'event_id','timestamp':'timestamp','location.long':'location_long','location.lat':'location_lat','individual.taxon.canonical.name':'individual_taxon_canonical_name','tag.local.identifier':'tag_local_identifier','individual.local.identifier (ID)':'individual_local_identifier_ID','study.name':'study_name','country':'country'})\n",
    "\n",
    "# Print features names\n",
    "data_cp.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_17528\\3021486648.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "# Parsing the timestamp column into a datetime format\n",
    "data_cp['timestamp'] = pd.to_datetime(data_cp['timestamp'])\n",
    "\n",
    "# Extract time of day, day of the week, month, etc., for temporal features.\n",
    "data_cp['hour'] = data_cp['timestamp'].dt.hour\n",
    "data_cp['day'] = data_cp['timestamp'].dt.day\n",
    "data_cp['month'] = data_cp['timestamp'].dt.month\n",
    "data_cp['year'] = data_cp['timestamp'].dt.year\n",
    "data_cp['dayofweek'] = data_cp['timestamp'].dt.dayofweek\n",
    "data_cp['date'] = data_cp['timestamp'].dt.date\n",
    "\n",
    "# Show unique values for day, month and year\n",
    "# print(data_cp['day'].unique())\n",
    "# print(data_cp['month'].unique())\n",
    "# print(data_cp['year'].unique())\n",
    "# print(data_cp['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'timestamp', 'location_long', 'location_lat',\n",
       "       'individual_taxon_canonical_name', 'tag_local_identifier',\n",
       "       'individual_local_identifier_ID', 'study_name', 'country', 'hour',\n",
       "       'day', 'month', 'year', 'dayofweek', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cp = data_cp.drop(['event_id', 'individual_local_identifier_ID', 'tag_local_identifier'], axis=1)\n",
    "\n",
    "data_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distances between consecutive events.\n",
    "### Calculating the distance between points using the Haversine Formula and group the data by tag_local_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guima\\AppData\\Local\\Temp\\ipykernel_17528\\4228301756.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location_long</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>individual_taxon_canonical_name</th>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th>individual_local_identifier_ID</th>\n",
       "      <th>study_name</th>\n",
       "      <th>country</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131891</th>\n",
       "      <td>131892.0</td>\n",
       "      <td>2015-10-12 21:00:00</td>\n",
       "      <td>-57.500308</td>\n",
       "      <td>-16.881923</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131939</th>\n",
       "      <td>131940.0</td>\n",
       "      <td>2015-10-14 23:00:00</td>\n",
       "      <td>-57.490634</td>\n",
       "      <td>-16.888566</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-14</td>\n",
       "      <td>1.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131945</th>\n",
       "      <td>131946.0</td>\n",
       "      <td>2015-10-15 05:00:00</td>\n",
       "      <td>-57.484631</td>\n",
       "      <td>-16.888427</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131984</th>\n",
       "      <td>131985.0</td>\n",
       "      <td>2015-10-16 20:00:00</td>\n",
       "      <td>-57.504718</td>\n",
       "      <td>-16.881299</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>2.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131989</th>\n",
       "      <td>131990.0</td>\n",
       "      <td>2015-10-17 02:01:00</td>\n",
       "      <td>-57.505744</td>\n",
       "      <td>-16.880184</td>\n",
       "      <td>Panthera onca</td>\n",
       "      <td>35957</td>\n",
       "      <td>117</td>\n",
       "      <td>Jaguar_Taiama</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             event_id           timestamp  location_long  \\\n",
       "tag_local_identifier                                                       \n",
       "35957                131891  131892.0 2015-10-12 21:00:00     -57.500308   \n",
       "                     131939  131940.0 2015-10-14 23:00:00     -57.490634   \n",
       "                     131945  131946.0 2015-10-15 05:00:00     -57.484631   \n",
       "                     131984  131985.0 2015-10-16 20:00:00     -57.504718   \n",
       "                     131989  131990.0 2015-10-17 02:01:00     -57.505744   \n",
       "\n",
       "                             location_lat individual_taxon_canonical_name  \\\n",
       "tag_local_identifier                                                        \n",
       "35957                131891    -16.881923                   Panthera onca   \n",
       "                     131939    -16.888566                   Panthera onca   \n",
       "                     131945    -16.888427                   Panthera onca   \n",
       "                     131984    -16.881299                   Panthera onca   \n",
       "                     131989    -16.880184                   Panthera onca   \n",
       "\n",
       "                            tag_local_identifier  \\\n",
       "tag_local_identifier                               \n",
       "35957                131891                35957   \n",
       "                     131939                35957   \n",
       "                     131945                35957   \n",
       "                     131984                35957   \n",
       "                     131989                35957   \n",
       "\n",
       "                             individual_local_identifier_ID     study_name  \\\n",
       "tag_local_identifier                                                         \n",
       "35957                131891                             117  Jaguar_Taiama   \n",
       "                     131939                             117  Jaguar_Taiama   \n",
       "                     131945                             117  Jaguar_Taiama   \n",
       "                     131984                             117  Jaguar_Taiama   \n",
       "                     131989                             117  Jaguar_Taiama   \n",
       "\n",
       "                            country  hour  day  month  year  dayofweek  \\\n",
       "tag_local_identifier                                                     \n",
       "35957                131891  Brazil    21   12     10  2015          0   \n",
       "                     131939  Brazil    23   14     10  2015          2   \n",
       "                     131945  Brazil     5   15     10  2015          3   \n",
       "                     131984  Brazil    20   16     10  2015          4   \n",
       "                     131989  Brazil     2   17     10  2015          5   \n",
       "\n",
       "                                   date  distance  \n",
       "tag_local_identifier                               \n",
       "35957                131891  2015-10-12       NaN  \n",
       "                     131939  2015-10-14     1.267  \n",
       "                     131945  2015-10-15     0.639  \n",
       "                     131984  2015-10-16     2.279  \n",
       "                     131989  2015-10-17     0.165  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fe = data_cp.copy()\n",
    "\n",
    "data_fe['timestamp'] = pd.to_datetime(data_fe['timestamp'])\n",
    "\n",
    "# Define Haversine formula for distance calculation\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return R * c\n",
    "\n",
    "# Function to calculate distance only for the same animal\n",
    "def calculate_distances_for_animal(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    group['distance'] = haversine(\n",
    "        group['location_lat'].shift(),\n",
    "        group['location_long'].shift(),\n",
    "        group['location_lat'],\n",
    "        group['location_long']\n",
    "    )\n",
    "    return group\n",
    "\n",
    "data_fe = data_fe.groupby('tag_local_identifier').apply(calculate_distances_for_animal)\n",
    "\n",
    "# Convert distance to kilometers\n",
    "data_fe['distance'] = data_fe['distance'] / 1000\n",
    "data_fe['distance'] = data_fe['distance'].round(3)\n",
    "\n",
    "data_fe.head()\n",
    "# data_fe.to_csv('DataS1/jaguar_movement_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Labeling the Data\n",
    " Behavior Labeling:\n",
    " Manually label a subset of data with different behaviors (e.g., movement, hunting, resting).\n",
    " Consider cross-referencing domain knowledge or complementary data to infer labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movement > 0.5: 'hunting' (highest priority)\n",
    "\n",
    "0.1 < velocity ≤ 0.5: 'movement'\n",
    "\n",
    "0.0 < velocity ≤ 0.1: 'slowing down'\n",
    "\n",
    "≤ 0.0: 'resting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             velocity      movement\n",
      "tag_local_identifier                               \n",
      "35957                131939    0.0070  slowing down\n",
      "                     131945    0.0296  slowing down\n",
      "                     131984    0.0162  slowing down\n",
      "                     131989    0.0076  slowing down\n",
      "                     132018    0.0146  slowing down\n",
      "...                               ...           ...\n",
      "TIAGO                106041    0.0252  slowing down\n",
      "                     106044    0.0001  slowing down\n",
      "                     106045    0.0063  slowing down\n",
      "                     106116    0.0046  slowing down\n",
      "                     106157    0.0088  slowing down\n",
      "\n",
      "[4890 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating velocity, direction, and movement features based on location data (location.long, location.lat).\n",
    "# time_diff = (data_cp['timestamp'] - data_cp['timestamp'].shift()).dt.total_seconds()\n",
    "\n",
    "# Ensuring the distance is in meters and the time differece is in seconds\n",
    "time_diff = data_fe['time_diff'] = data_fe['timestamp'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "data_fe['distance'] = data_fe['distance'] * 1000\n",
    "time_diff_seconds = data_fe['time_diff'] * 3600\n",
    "\n",
    "# print(data_fe['time_diff'].head())\n",
    "\n",
    "data_fe['velocity'] = data_fe['distance'] / time_diff_seconds\n",
    "data_fe['velocity'] = data_fe['velocity'].round(4)\n",
    "data_fe = data_fe.dropna(subset=[ 'distance', 'velocity'])\n",
    "\n",
    "data_fe['direction'] = np.arctan2(data_fe['location_long'] - data_fe['location_long'].shift(), data_fe['location_lat'] - data_fe['location_lat'].shift())\n",
    "\n",
    "data_fe['movement'] = np.where(\n",
    "    data_fe['velocity'] > 0.5, 'hunting',\n",
    "    np.where(\n",
    "        data_fe['velocity'] > 0.1, 'movement',\n",
    "        np.where(\n",
    "            data_fe['velocity'] > 0.0, 'slowing down',\n",
    "            'resting'  # Velocity <= 0.0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# data_cp = data_cp.dropna()\n",
    "print(data_fe[['velocity', 'movement']])\n",
    "# print(data_fe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive acceleration or changes in movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acceleration</th>\n",
       "      <th>change_in_direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_local_identifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">35957</th>\n",
       "      <th>131984</th>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-2.7774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131989</th>\n",
       "      <td>-0.0014</td>\n",
       "      <td>0.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132018</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.3062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132026</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132056</th>\n",
       "      <td>0.0003</td>\n",
       "      <td>3.0119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             acceleration  change_in_direction\n",
       "tag_local_identifier                                          \n",
       "35957                131984       -0.0003              -2.7774\n",
       "                     131989       -0.0014               0.4860\n",
       "                     132018        0.0002              -0.3062\n",
       "                     132026        0.0005               0.1463\n",
       "                     132056        0.0003               3.0119"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate acceleration as the change in velocity over time\n",
    "data_fe['acceleration'] = data_fe['velocity'].diff() / data_fe['time_diff']\n",
    "\n",
    "# Calculate change in direction\n",
    "data_fe['change_in_direction'] = data_fe['direction'].diff()\n",
    "\n",
    "# Drop rows with NaN values in acceleration and change_in_direction\n",
    "data_fe = data_fe.dropna(subset=['acceleration', 'change_in_direction'])\n",
    "\n",
    "# Round the values for better readability\n",
    "data_fe['acceleration'] = data_fe['acceleration'].round(4)\n",
    "data_fe['change_in_direction'] = data_fe['change_in_direction'].round(4)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "data_fe[['acceleration', 'change_in_direction']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103]\n"
     ]
    }
   ],
   "source": [
    "# Encode tag_local_identifier to numerical values\n",
    "\n",
    "data_fe['tag_local_identifier'] = pd.Categorical(data_fe['tag_local_identifier'])\n",
    "data_fe['tag_local_identifier'] = data_fe['tag_local_identifier'].cat.codes\n",
    "\n",
    "print(data_fe['tag_local_identifier'].unique())\n",
    "# data_encoded = pd.get_dummies(data_fe, columns=['tag_local_identifier'])\n",
    "# data_encoded.columns\n",
    "# data_encoded['tag_local_identifier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to get temporal features from Paraguay, Brazil, Costa Rica, Argentina and Mexico between 1/1/1999 and 31/12/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.7)\n",
      "(553652, 127)\n",
      "(4169, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get World  Climate data\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"christopherlemke/monthly-climat-reports-from-stations-worldwide\")\n",
    "\n",
    "reports_data = pd.read_csv(path + '/dwd-cdc_CLIMAT_reports_stations_ww.csv')\n",
    "stations_data = pd.read_csv(path + '/dwd-cdc_station_data_ww.csv')\n",
    "\n",
    "# Save the data to a csv file\n",
    "reports_data.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide.csv', index=False)\n",
    "stations_data.to_csv('DataS1/dwd-cdc_station_data_ww.csv', index=False)\n",
    "\n",
    "print(reports_data.shape)\n",
    "print(stations_data.shape)\n",
    "\n",
    "reports_data = reports_data.sample(SAMPLE_SIZE)\n",
    "# stations_data = stations_data.sample(SAMPLE_SIZE)\n",
    "\n",
    "# print(reports_data.head())\n",
    "# print(stations_data.head())\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Reports data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farenheit_to_celsius(farenheit):\n",
    "    return (farenheit - 32) * 5.0/9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Engineering\n",
    " Environmental Features:\n",
    " Integrate meteorological data (e.g., temperature, rainfall) if available.\n",
    " Integrate terrain data (e.g., elevation, land cover type) if available.\n",
    " Temporal Features:\n",
    " Extract time of day, day of week, and season from the timestamp.\n",
    " Identify specific behavioral periods like night vs. day.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthly_mean_air_temperature          0\n",
      "mean_daily_maximum_air_temperature    0\n",
      "mean_daily_minimum_air_temperature    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>station_id</th>\n",
       "      <th>G1</th>\n",
       "      <th>G1.1</th>\n",
       "      <th>G1.2</th>\n",
       "      <th>sn</th>\n",
       "      <th>monthly_mean_air_temperature</th>\n",
       "      <th>G1.3</th>\n",
       "      <th>sn.1</th>\n",
       "      <th>...</th>\n",
       "      <th>iw</th>\n",
       "      <th>fx</th>\n",
       "      <th>yfx</th>\n",
       "      <th>G4.6</th>\n",
       "      <th>Dts</th>\n",
       "      <th>Dgr</th>\n",
       "      <th>G4.7</th>\n",
       "      <th>iy</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>530511</th>\n",
       "      <td>2004</td>\n",
       "      <td>6</td>\n",
       "      <td>72421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466209</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>89865</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.44</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22175</th>\n",
       "      <td>2006</td>\n",
       "      <td>9</td>\n",
       "      <td>20069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402833</th>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>6590</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128774</th>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>44225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  station_id   G1  G1.1  G1.2   sn  \\\n",
       "530511  2004      6       72421  NaN   2.0   3.0  0.0   \n",
       "466209  2019     11       89865  1.0   NaN   3.0  1.0   \n",
       "22175   2006      9       20069  1.0   2.0   3.0  1.0   \n",
       "402833  2009      8        6590  1.0   2.0   3.0  0.0   \n",
       "128774  2011      3       44225  1.0   2.0   3.0  1.0   \n",
       "\n",
       "        monthly_mean_air_temperature  G1.3  sn.1  ...   iw     fx   yfx  G4.6  \\\n",
       "530511                        102.22   4.0   0.0  ...  4.0    NaN   NaN   6.0   \n",
       "466209                         24.44   4.0   1.0  ...  NaN    NaN   NaN   NaN   \n",
       "22175                          -2.78   4.0   1.0  ...  NaN    NaN   NaN   NaN   \n",
       "402833                         90.56   4.0   0.0  ...  1.0  140.0  78.0   6.0   \n",
       "128774                         95.56   4.0   1.0  ...  1.0  180.0  21.0   NaN   \n",
       "\n",
       "        Dts  Dgr  G4.7   iy    Gx    Gn  \n",
       "530511  7.0  0.0   NaN  NaN   NaN   NaN  \n",
       "466209  NaN  NaN   NaN  NaN   NaN   NaN  \n",
       "22175   NaN  NaN   NaN  NaN   NaN   NaN  \n",
       "402833  2.0  0.0   7.0  2.0  24.0  24.0  \n",
       "128774  NaN  NaN   NaN  NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove columns\n",
    "reports_data_dropped = reports_data.drop(['Po', 'Po.1', 'P', 'P.1', 'T.1', 'st', 'st.1', \n",
    "                                          'Tx.1', 'Tn.1', \n",
    "                                          'e', 'e.1', \n",
    "                                          'R1.1', 'nr.1', 'S1.1', 'Rd', 'mp', 'mT', 'mTx', 'mTn', 'me', 'mR', 'mS', \n",
    "                                            #   'sn', 'sn.1', 'sn.2', 'sn.3', 'sn.4', 'sn.5', 'sn.6', 'sn.7', 'sn.8', 'sn.9',\n",
    "                                            # 'G1', 'G1.1', 'G1.2', 'G1.3', 'G1.4', 'G1.5', 'G1.6', 'G1.7', 'G1.8',\n",
    "                                            # 'G2', 'G2.1', 'G2.2', 'G2.3', 'G2.4', 'G2.5', 'G2.6', 'G2.7', 'G2.8', 'G2.9',\n",
    "                                            # 'G3', 'G3.1', 'G3.2', 'G3.3', 'G3.4', 'G3.5', 'G3.6', 'G3.7', 'G3.8', 'G3.9',\n",
    "                                            # 'G4', 'G4.1', 'G4.2', 'G4.3', 'G4.4', 'G4.5', 'G4.6', 'G4.7',\n",
    "                                            'Yb', 'Yc', 'P', 'YP', 'YR', 'YS', 'YT', 'YTx',\n",
    "                                            'Ye', 'G4', 'Txd', 'yx', 'Tnd', 'Tax', 'Tan' ], axis=1)\n",
    "\n",
    "# Drop columns with no data\n",
    "reports_data_dropped = reports_data_dropped.dropna(axis=1, how='all')\n",
    "\n",
    "# Rename columns\n",
    "reports_data_renamed = reports_data_dropped.rename(columns={'IIiii':'station_id', 'T':'monthly_mean_air_temperature', \n",
    "                                            'Tx':'mean_daily_maximum_air_temperature', \n",
    "                                            'Tn':'mean_daily_minimum_air_temperature', \n",
    "                                            'R1':'total_precipitation_month', 'S1':'total_sunshine_month', \n",
    "                                            'ps':'percentage_total_sunshine_duration_relative_normal', \n",
    "                                            'P0':'monthly_mean_pressure_station_level', 'e':'mean_vapor_pressure_month', \n",
    "                                            'nr':'number_days_month_precipitation', \n",
    "                                            'yP':'missing_years_air_pressure', 'yR':'missing_years_precipitation', 'yS':'missing_years_sunshine_duration', \n",
    "                                            'yT':'missing_years_mean_air_temperature', 'yTx':'missing_years_mean_extreme_air_temperature', 'ye':'missing_years_vapor_pressure', \n",
    "                                            'T25':'days_month_maximum_air_temperature_25', 'T30':'days_month_maximum_air_temperature_30', \n",
    "                                            'T35':'days_month_maximum_air_temperature_35', 'T40':'days_month_maximum_air_temperature_40', \n",
    "                                            'Tn0':'days_month_minimum_air_temperature_0', 'Tx0':'days_month_maximum_air_temperature_0', 'R01':'days_month_precipitation_1', \n",
    "                                            'R05':'days_month_precipitation_5', 'R10':'days_month_precipitation_10', 'R50':'days_month_precipitation_50', \n",
    "                                            'R100':'days_month_precipitation_100', 'R150':'days_month_precipitation_150', 's00':'days_month_snow_depth_0', \n",
    "                                            's01':'days_month_snow_depth_1', 's10':'days_month_snow_depth_10', 's50':'days_month_snow_depth_50', 'f10':'days_month_wind_speed_10', \n",
    "                                            'f20':'days_month_wind_speed_20', 'f30':'days_month_wind_speed_30', 'V1':'days_month_visibility_50', 'V2':'days_month_visibility_100', \n",
    "                                            'V3':'days_month_visibility_1000', 'yn': 'day_lowest_daily_mean_air_temperature_month', 'yax': 'day_highest_daily_mean_air_temperature_month', 'yan':'day_lowest_air_tempreature_month',\n",
    "                                            'Rx':'highest_daily_amount_precipitation_month_tenths_mm',  \n",
    "                                            # 'yr':'day_highest_daily_amount_precipitation_month'\n",
    "                                            })\n",
    "\n",
    "# Remove rows with missing values in [year]\n",
    "reports_data_renamed = reports_data_renamed.dropna(subset=['year'])\n",
    "\n",
    "# Enconde Year, Month, and Station ID to integer\n",
    "reports_data_renamed['year'] = reports_data_renamed['year'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['month'] = reports_data_renamed['month'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['station_id'] = reports_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "reports_data_renamed['sn'] = reports_data_renamed['sn'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Convert temperature features to Celsius\n",
    "# °C = (°F - 32) × 5/9\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = farenheit_to_celsius(reports_data_renamed['monthly_mean_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] =  farenheit_to_celsius(reports_data_renamed['mean_daily_maximum_air_temperature']).round(2)\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = farenheit_to_celsius(reports_data_renamed['mean_daily_minimum_air_temperature']).round(2)\n",
    "\n",
    "# Check for missing values in temperature features\n",
    "# print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean\n",
    "reports_data_renamed['monthly_mean_air_temperature'] = reports_data_renamed['monthly_mean_air_temperature'].fillna(reports_data_renamed['monthly_mean_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_maximum_air_temperature'] = reports_data_renamed['mean_daily_maximum_air_temperature'].fillna(reports_data_renamed['mean_daily_maximum_air_temperature'].mean())\n",
    "reports_data_renamed['mean_daily_minimum_air_temperature'] = reports_data_renamed['mean_daily_minimum_air_temperature'].fillna(reports_data_renamed['mean_daily_minimum_air_temperature'].mean())\n",
    "\n",
    "print(reports_data_renamed[['monthly_mean_air_temperature', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature']].isnull().sum())\n",
    "\n",
    "# # Show null values\n",
    "# print(reports_data_renamed.isnull().sum())\n",
    "reports_data_renamed.to_csv('DataS1/monthly-climat-reports-from-stations-worldwide-cleaned.csv', index=False)\n",
    "\n",
    "reports_data_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_data.dtypes\n",
    "\n",
    "# Rename columns\n",
    "# 0: Station ID, 1: Station Name, 2: Latitude, 3: Longitude, 4:Height, 5: Country\n",
    "stations_data_renamed = stations_data.rename(columns={'0':'station_id', '1':'station_name', '2':'latitude', '3':'longitude', '4':'height', '5':'country'})\n",
    "stations_data_renamed['station_id'] = stations_data_renamed['station_id'].str.strip()\n",
    "stations_data_renamed = stations_data_renamed[:-1]\n",
    "\n",
    "# print(stations_data_renamed['station_id'].unique())\n",
    "\n",
    "\n",
    "stations_data_renamed['station_id'] = (\n",
    "    stations_data_renamed['station_id']\n",
    "    .str.replace(r'\\D', '', regex=True)  # Remove all non-digit characters\n",
    "    .pipe(pd.to_numeric, errors='coerce')  # Convert to numeric, invalid become NaN\n",
    "    .astype('Int64')  # Convert to pandas' nullable Int64 type\n",
    ")\n",
    "\n",
    "\n",
    "# stations_data_renamed['station_id'] = stations_data_renamed['station_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "# stations_data_renamed.head()\n",
    "# stations_data_renamed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'month', 'station_id', 'G1', 'G1.1', 'G1.2', 'sn',\n",
      "       'monthly_mean_air_temperature', 'G1.3', 'sn.1',\n",
      "       'mean_daily_maximum_air_temperature', 'sn.2',\n",
      "       'mean_daily_minimum_air_temperature', 'G1.4', 'G1.5',\n",
      "       'total_precipitation_month', 'number_days_month_precipitation', 'G1.6',\n",
      "       'total_sunshine_month',\n",
      "       'percentage_total_sunshine_duration_relative_normal', 'G1.7', 'G1.8',\n",
      "       'G2', 'G2.1', 'G2.2', 'G2.3', 'sn.3', 'G2.4', 'sn.4', 'sn.5', 'G2.5',\n",
      "       'G2.6', 'G2.7', 'G2.8', 'G2.9', 'G3',\n",
      "       'days_month_maximum_air_temperature_25',\n",
      "       'days_month_maximum_air_temperature_30', 'G3.1',\n",
      "       'days_month_maximum_air_temperature_35',\n",
      "       'days_month_maximum_air_temperature_40', 'G3.2',\n",
      "       'days_month_minimum_air_temperature_0',\n",
      "       'days_month_maximum_air_temperature_0', 'G3.3',\n",
      "       'days_month_precipitation_1', 'days_month_precipitation_5', 'G3.4',\n",
      "       'days_month_precipitation_10', 'days_month_precipitation_50', 'G3.5',\n",
      "       'days_month_precipitation_100', 'days_month_precipitation_150', 'G3.6',\n",
      "       'days_month_snow_depth_0', 'days_month_snow_depth_1', 'G3.7',\n",
      "       'days_month_snow_depth_10', 'days_month_snow_depth_50', 'G3.8',\n",
      "       'days_month_wind_speed_10', 'days_month_wind_speed_20',\n",
      "       'days_month_wind_speed_30', 'G3.9', 'days_month_visibility_50',\n",
      "       'days_month_visibility_100', 'days_month_visibility_1000', 'sn.6',\n",
      "       'G4.1', 'sn.7', 'day_lowest_daily_mean_air_temperature_month', 'G4.2',\n",
      "       'sn.8', 'day_highest_daily_mean_air_temperature_month', 'G4.3', 'sn.9',\n",
      "       'day_lowest_air_tempreature_month', 'G4.4',\n",
      "       'highest_daily_amount_precipitation_month_tenths_mm', 'yr', 'G4.5',\n",
      "       'iw', 'fx', 'yfx', 'G4.6', 'Dts', 'Dgr', 'G4.7', 'iy', 'Gx', 'Gn',\n",
      "       'station_name', 'latitude', 'longitude', 'height', 'country'],\n",
      "      dtype='object')\n",
      "(4888, 22)\n",
      "   year  month  station_id                   country  \\\n",
      "0  2004      6       72421  United States of America   \n",
      "1  2019     11       89865  United States of America   \n",
      "2  2009      8        6590                Luxembourg   \n",
      "3  2011      3       44225                  Mongolia   \n",
      "4  2006      5       52681                     China   \n",
      "\n",
      "   mean_daily_maximum_air_temperature  mean_daily_minimum_air_temperature  \\\n",
      "0                              131.67                               72.78   \n",
      "1                                8.89                               37.78   \n",
      "2                              119.44                               60.00   \n",
      "3                               43.33                              136.67   \n",
      "4                              121.11                               41.67   \n",
      "\n",
      "   total_precipitation_month  number_days_month_precipitation  \\\n",
      "0                       75.0                              8.0   \n",
      "1                        NaN                              NaN   \n",
      "2                       49.0                              6.0   \n",
      "3                        3.0                              1.0   \n",
      "4                       18.0                              2.0   \n",
      "\n",
      "   total_sunshine_month  percentage_total_sunshine_duration_relative_normal  \\\n",
      "0                   NaN                                                NaN    \n",
      "1                   NaN                                                NaN    \n",
      "2                 288.0                                              127.0    \n",
      "3                   NaN                                                NaN    \n",
      "4                 326.0                                              112.0    \n",
      "\n",
      "   ...  days_month_snow_depth_1  days_month_snow_depth_10  \\\n",
      "0  ...                      NaN                       NaN   \n",
      "1  ...                      NaN                       NaN   \n",
      "2  ...                      0.0                       0.0   \n",
      "3  ...                     31.0                      31.0   \n",
      "4  ...                      NaN                       NaN   \n",
      "\n",
      "   days_month_snow_depth_50  days_month_visibility_50  \\\n",
      "0                       NaN                       NaN   \n",
      "1                       NaN                       NaN   \n",
      "2                       0.0                       0.0   \n",
      "3                       0.0                       NaN   \n",
      "4                       NaN                       0.0   \n",
      "\n",
      "   days_month_visibility_100  days_month_visibility_1000  \\\n",
      "0                        NaN                         NaN   \n",
      "1                        NaN                         NaN   \n",
      "2                        0.0                         1.0   \n",
      "3                        NaN                         NaN   \n",
      "4                        0.0                         2.0   \n",
      "\n",
      "   day_lowest_air_tempreature_month  \\\n",
      "0                               5.0   \n",
      "1                               1.0   \n",
      "2                              30.0   \n",
      "3                               3.0   \n",
      "4                               9.0   \n",
      "\n",
      "   day_highest_daily_mean_air_temperature_month  \\\n",
      "0                                          58.0   \n",
      "1                                          24.0   \n",
      "2                                          20.0   \n",
      "3                                          29.0   \n",
      "4                                          31.0   \n",
      "\n",
      "   day_lowest_air_tempreature_month  \\\n",
      "0                               5.0   \n",
      "1                               1.0   \n",
      "2                              30.0   \n",
      "3                               3.0   \n",
      "4                               9.0   \n",
      "\n",
      "   highest_daily_amount_precipitation_month_tenths_mm  \n",
      "0                                               41.0   \n",
      "1                                                NaN   \n",
      "2                                              220.0   \n",
      "3                                               18.0   \n",
      "4                                               85.0   \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Data successfully loaded into SQLite database!\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Merge the reports and stations data by station_id\n",
    "import sqlite3\n",
    "\n",
    "data_merged = pd.merge(reports_data_renamed, stations_data_renamed, on='station_id')\n",
    "\n",
    "# data_fe['country'].unique()\n",
    "\n",
    "data_merged['country'] = data_merged['country'].str.strip()\n",
    "\n",
    "# print(data_merged['country'].unique())\n",
    "\n",
    "# data_merged['country'].isnull().sum()\n",
    "\n",
    "# print(data_merged.columns)\n",
    "\n",
    "month_daily_data = data_merged[['year', 'month', 'station_id', 'country', 'mean_daily_maximum_air_temperature', 'mean_daily_minimum_air_temperature', 'total_precipitation_month', \n",
    "'number_days_month_precipitation', 'total_sunshine_month', 'percentage_total_sunshine_duration_relative_normal', 'days_month_maximum_air_temperature_25', \n",
    "'days_month_maximum_air_temperature_30', 'days_month_maximum_air_temperature_35', 'days_month_maximum_air_temperature_40', 'days_month_minimum_air_temperature_0', \n",
    "'days_month_maximum_air_temperature_0', 'days_month_precipitation_1', 'days_month_precipitation_5', 'days_month_precipitation_10', 'days_month_precipitation_50', \n",
    "'days_month_precipitation_100', 'days_month_precipitation_150', 'days_month_snow_depth_0', 'days_month_snow_depth_1', 'days_month_snow_depth_10', 'days_month_snow_depth_50', \n",
    "'days_month_visibility_50', 'days_month_visibility_100', 'days_month_visibility_1000', 'day_lowest_air_tempreature_month', 'day_highest_daily_mean_air_temperature_month', 'day_lowest_air_tempreature_month', 'highest_daily_amount_precipitation_month_tenths_mm']]\n",
    "\n",
    "# print(data_fe.shape)\n",
    "\n",
    "# print(month_daily_data.head())\n",
    "\n",
    "# data_merged.head()\n",
    "\n",
    "conn = sqlite3.connect('jaguar_data.db')\n",
    "\n",
    "# Save data to SQLite database\n",
    "reports_data_renamed.to_sql('reports', conn, if_exists='replace', index=False)\n",
    "stations_data_renamed.to_sql('stations', conn, if_exists='replace', index=False)\n",
    "# data_merged.to_sql('reports_country', conn, if_exists='replace', index=False)\n",
    "data_fe.to_sql('data_fe', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data successfully loaded into SQLite database!\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Merge data_merged with data_fe only by the countries in data_fe\n",
    "data_merged_countries = pd.merge(data_fe, month_daily_data, on='country')\n",
    "\n",
    "# data_merged_countries = pd.merge(data_merged, data_fe, on='country', how='inner')\n",
    "# data_merged_countries = data_merged[data_merged['country'].isin(data_fe['country'].unique())]\n",
    "# data_merged_countries.head()\n",
    "\n",
    "print(\"Database connection closed.\")\n",
    "\n",
    "# Save\n",
    "data_merged_countries.to_csv('DataS1/jaguar_movement_with_countries_climate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Selection\n",
    " Choose Algorithms:\n",
    " Evaluate models like Random Forest, Gradient Boosting Machines (e.g., XGBoost), or neural networks (RNN/LSTM for sequential data).\n",
    " Decide on the best model based on data type and problem complexity.\n",
    " Split Data:\n",
    " Split the data into training and testing sets (e.g., 80/20 or 70/30 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Data Preparation with Country Considerations\n",
    "\n",
    "# Defining target and features \n",
    "\n",
    "target = 'movement'\n",
    "features = [col for col in data_fe.columns if col not in [target, 'event_id', 'timestamp', 'date']]\n",
    "categorical_features = ['country', 'study_name']\n",
    "numeric_features = [col for col in features if col not in categorical_features + [target]]\n",
    "data_fe['stratify_col'] = data_fe['country'] + '_' + data_fe[target]\n",
    "\n",
    "# Filter out classes with fewer than two samples\n",
    "class_counts = data_fe['stratify_col'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "data_fe_filtered = data_fe[data_fe['stratify_col'].isin(valid_classes)]\n",
    "\n",
    "# Splitting the data with stratification by both country and movement class\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_fe_filtered[features], \n",
    "    data_fe_filtered[target],\n",
    "    test_size=0.2,\n",
    "    stratify=data_fe_filtered['stratify_col'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model Selection\n",
    "models = {\n",
    "    # Handles categorical features natively, good for country-wise patterns\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        cat_features=categorical_features,\n",
    "        auto_class_weights='Balanced',\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    ),\n",
    "    \n",
    "    # Baseline for comparison\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    # Gradient boosting alternative\n",
    "    'XGBoost': XGBClassifier(\n",
    "        scale_pos_weight='balanced',\n",
    "        enable_categorical=False,  # Requires pre-encoded categories\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Training and Evaluation\n",
    " Train Models:\n",
    " Train selected models using the preprocessed data.\n",
    " Use cross-validation to tune model parameters.\n",
    " Evaluate Model:\n",
    " Evaluate performance using metrics like accuracy, precision, recall, and F1-score.\n",
    " If the dataset is imbalanced, apply techniques like SMOTE to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "Bad value for num_feature[non_default_doc_idx=0,feature_idx=2]=\"Panthera onca\": Cannot convert 'b'Panthera onca'' to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m_catboost.pyx:2547\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:1226\u001b[0m, in \u001b[0;36m_catboost._FloatOrNan\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:1021\u001b[0m, in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert 'b'Panthera onca'' to float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     X_train[categorical_features] \u001b[38;5;241m=\u001b[39m X_train[categorical_features]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      8\u001b[0m     X_test[categorical_features] \u001b[38;5;241m=\u001b[39m X_test[categorical_features]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# One-hot encode categorical features for other models\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     X_train_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X_train, columns\u001b[38;5;241m=\u001b[39mcategorical_features)\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:5245\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5243\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5246\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5247\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:2395\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES \u001b[38;5;241m+\u001b[39m (Pool,)):\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2395\u001b[0m train_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_train_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2405\u001b[0m params \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2406\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:2275\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2272\u001b[0m text_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2273\u001b[0m embedding_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2275\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m \u001b[43m_build_train_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2276\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2277\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_pool\u001b[38;5;241m.\u001b[39mis_empty_:\n\u001b[0;32m   2279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:1513\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1512\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1513\u001b[0m     train_pool \u001b[38;5;241m=\u001b[39m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_pool\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:855\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, graph, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[0;32m    849\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    850\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    851\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    852\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    853\u001b[0m             )\n\u001b[1;32m--> 855\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_can_be_none:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Guima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:1491\u001b[0m, in \u001b[0;36mPool._init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m     feature_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_transform_tags(feature_tags, feature_names)\n\u001b[1;32m-> 1491\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m_catboost.pyx:4339\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4391\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4200\u001b[0m, in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:3127\u001b[0m, in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:2591\u001b[0m, in \u001b[0;36m_catboost.create_num_factor_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:2549\u001b[0m, in \u001b[0;36m_catboost.get_float_feature\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=2]=\"Panthera onca\": Cannot convert 'b'Panthera onca'' to float"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    # Special handling for CatBoost\n",
    "    if name == 'CatBoost':\n",
    "        # Convert categorical columns to string type\n",
    "        X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "        X_test[categorical_features] = X_test[categorical_features].astype(str)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        # One-hot encode categorical features for other models\n",
    "        X_train_encoded = pd.get_dummies(X_train, columns=categorical_features)\n",
    "        X_test_encoded = pd.get_dummies(X_test, columns=categorical_features)\n",
    "        \n",
    "        # Align columns between train and test\n",
    "        X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "        \n",
    "        model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    # Cross-validate with regional stratification\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train if name == 'CatBoost' else X_train_encoded,\n",
    "        cv=5, scoring='f1_weighted',\n",
    "        groups=data.loc[X_train.index, 'country']  # Keep countries together in folds\n",
    "    )\n",
    "    print(f\"CV F1-weighted: {np.mean(cv_scores):.3f} ± {np.std(cv_scores):.3f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if name == 'CatBoost':\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_encoded)\n",
    "    \n",
    "    print(\"Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Cross-Referencing with Complementary Data\n",
    " Integrate Meteorological Data:\n",
    " Merge weather data (e.g., temperature, humidity) with animal tracking data.\n",
    " Integrate Terrain Data:\n",
    " Merge terrain data (e.g., elevation, land use) with animal tracking data.\n",
    " Behavioral Biology Data:\n",
    " Incorporate knowledge from related behavioral biology studies (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Model Deployment\n",
    " Model Evaluation: Confirm that the model generalizes well to new or unseen data.\n",
    " Deploy Model:\n",
    " Prepare the model for deployment in a real-time or batch setting for wildlife tracking.\n",
    " Implement a user interface or tool to apply the model to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
